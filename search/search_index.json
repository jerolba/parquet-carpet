{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Carpet: Parquet Serialization and Deserialization Library for Java","text":"<p>A Java library for serializing and deserializing Parquet files efficiently using Java records. This library provides a simple and user-friendly API for working with Parquet files, making it easy to read and write data in the Parquet format in your Java applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Serialize Java records to Parquet files</li> <li>Deserialize Parquet files to Java records</li> <li>Support nested data structures</li> <li>Support nested Collections and Maps</li> <li>Very simple API</li> <li>Low level configuration of Parquet properties</li> <li>Low overhead processing files</li> <li>Minimized <code>parquet-java</code> and hadoop transitive dependencies</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Add the dependency to your project:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.5.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>implementation 'com.jerolba:carpet-record:0.5.0'\n</code></pre> <p>Write and read your data:</p> <pre><code>// Define your data structure\nrecord MyRecord(long id, String name, int size, double value) { }\n\n// Write to Parquet\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (var writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n\n// Read from Parquet\nList&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre> <p>Check out the Getting Started guide for more details.</p>"},{"location":"advanced/column-mapping/","title":"Column Name Mapping","text":"<p>Carpet uses reflection to discover the schema of your files. Since Java record attribute names are limited by Java syntax while Parquet column names are more flexible.</p>"},{"location":"advanced/column-mapping/#column-name-aliasing","title":"Column Name Aliasing","text":"<p>You can use the <code>@Alias</code> annotation to specify a different name for a field in the Parquet schema. This is useful when you want to map a Java field to a Parquet column with a different name or format.</p> <pre><code>record MyRecord(long id, String name, @Alias(\"$name.id\") String nameId) { }\n</code></pre>"},{"location":"advanced/column-mapping/#column-name-conversion","title":"Column Name Conversion","text":"<p>Carpet supports automatic conversion of Java field names to Parquet column names. By default, it uses the same name as the field. However, you can modify this behaviour while configuring Carpet.</p>"},{"location":"advanced/column-mapping/#writing","title":"Writing","text":"<p>Writing a file, configure the property <code>columnNamingStrategy</code>:</p> <pre><code>record MyRecord(long userCode, String userName) { }\n\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n    .withColumnNamingStrategy(ColumnNamingStrategy.SNAKE_CASE)\n    .build()) {\n  writer.write(data);\n}\n</code></pre> <p>This will create a Parquet file with columns named <code>user_code</code> and <code>user_name</code>:</p> <pre><code>message MyRecord {\n  required int64 user_code;\n  optional binary user_name (STRING);\n}\n</code></pre> <p>At the moment, only the snake conversion strategy is implemented.</p>"},{"location":"advanced/column-mapping/#reading","title":"Reading","text":"<p>To read a file using the inverse logic we must configure the property <code>fieldMatchingStrategy</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(input, SomeEntity.class)\n    .withFieldMatchingStrategy(FieldMatchingStrategy.SNAKE_CASE);\nList&lt;SomeEntity&gt; list = reader.toList();\n</code></pre> <p>Available strategies reading a file are:</p> <ul> <li><code>FIELD_NAME</code>: Match column with exact field name</li> <li><code>SNAKE_CASE</code>: Match column with snake_case version of field name</li> <li><code>BEST_EFFORT</code>: Try exact match first, then try snake_case</li> </ul> <p>Reading and writing a file, @Alias annotation has precedence over the strategy configuration.</p>"},{"location":"advanced/configuration/","title":"Advanced Configuration","text":"<p>Parquet library provides multiple configuration options to customize the behavior of the writer and reader. <code>CarpetWriter</code> and <code>CarpetReader</code> hide most of these options, but you can still access them if needed.</p> <p>On the other hand, <code>Carpet</code> requires some optional configuration to setup how to handle specific types, such as <code>BigDecimal</code> and <code>LocalDateTime</code>.</p> <p>This section will cover the advanced configurations available in Carpet.</p>"},{"location":"advanced/configuration/#writer-configuration","title":"Writer Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration","title":"Parquet Configuration","text":"<p>Default <code>CarpetWriter</code> constructors cover default <code>ParquetWriter</code> configuration. You can customize Parquet configuration using <code>CarpetWriter.Builder</code>, that exposes all configuration methods if you need to tune it (compression, sizes, hadoop usage, etc).</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n    .withWriteMode(Mode.OVERWRITE)\n    .withCompressionCodec(CompressionCodecName.GZIP)\n    .withPageRowCountLimit(100_000)\n    .withBloomFilterEnabled(\"name\", true)\n    .build()) {\nwriter.write(data);\n</code></pre> <p>Any <code>ParquetWriter</code> configuration can be set using the <code>CarpetWriter.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration","title":"Carpet Configuration","text":"<p>Carpet provides some global configuration options to customize the default behavior of the writer managing some types.</p>"},{"location":"advanced/configuration/#bigdecimal-precision-and-scale","title":"BigDecimal precision and scale","text":"<p>DECIMAL type requires specifying both precision and scale when persisting values. This configuration is set globally when writing a file:</p> <pre><code>record MyRecord(String id, String name, BigDecimal price) { }\n\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(precision, scale)\n        .build()) {\n</code></pre> <p>There is no default value. If <code>BigDecimal</code> type is encountered, but precision and scale are not configured, Carpet throws an exception.</p> <p>If a <code>BigDecimal</code> value has a higher scale than the configured scale, Carpet does not rescale it by default and instead it throws an exception. To prevent this and automatically rescale values to the configured scale, you must specify the <code>RoundingMode</code> using the <code>withBigDecimalScaleAdjustment</code> method:</p> <pre><code>try (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(20, 3)\n        .withBigDecimalScaleAdjustment(RoundingMode.HALF_UP)\n        .build()) {\n    writer.write(new MyRecord(\"1\", \"item1\", new BigDecimal(\"123.45678\")));\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>BigDecimal</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#time-unit-configuration","title":"Time-Unit Configuration","text":"<p>TIME and TIMESTAMP Parquet types support configuring the decimal second unit (<code>MILLIS</code>, <code>MICROS</code> or <code>NANOS</code>).</p> <p>In Carpet, Time-Unit configuration is global when writing a file, and by default it's configured as <code>MILLIS</code>.</p> <p>The global configuration can be overwritten in the CarpetWriter builder:</p> <pre><code>record MyRecord(long itemId, int count, LocalTime saleTime) { }\n\nvar writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n        .withDefaultTimeUnit(TimeUnit.MICROS);\n        .build()) {\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>LocalTime</code>, <code>LocalDateTime</code> and <code>Instant</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#reader-configuration","title":"Reader Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration_1","title":"Parquet Configuration","text":"<p>CarpetReader doesn't provide a builder. It has been simplified to just provide Carpet specific configuration. You can still access all <code>ParquetReader</code> configuration options using the <code>CarpetParquetReader.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration_1","title":"Carpet Configuration","text":"<p>CarpetReader provides some configuration options to customize the behavior of the reader matching the schema of the file with the schema of the class used to read it.</p> <p>Configure how schema mismatches are handled:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    // Fail on null values for primitives\n    .withFailOnNullForPrimitives(true)\n    // Allow missing columns in the file\n    .withFailOnMissingColumn(false)\n    // Prevent narrowing conversions\n    .withFailNarrowingPrimitiveConversion(true)\n    // Flexible name matching\n    .withFieldMatchingStrategy(FieldMatchingStrategy.BEST_EFFORT);\n</code></pre>"},{"location":"advanced/data-types/","title":"Supported Data Types","text":""},{"location":"advanced/data-types/#basic-types","title":"Basic Types","text":"<p>Carpet maps main Java types to Parquet data types automatically:</p> Java Type Parquet Type boolean/Boolean boolean byte/Byte int32 short/Short int32 int/Integer int32 long/Long int64 float/Float float double/Double double Binary binary String binary (STRING) Enum binary (ENUM) UUID fixed_len_byte_array(16) (UUID)"},{"location":"advanced/data-types/#temporal-types","title":"Temporal Types","text":"<p>Support for Java time types:</p> Java Type Parquet Type LocalDate int32 (DATE) LocalTime int32 (TIME(MILLIS|MICROS)) or int64 (TIME(NANOS)) LocalDateTime int64 (TIMESTAMP(MILLIS|MICROS|NANOS)) Instant int64 (TIMESTAMP(MILLIS|MICROS|NANOS))"},{"location":"advanced/data-types/#decimal-numbers","title":"Decimal Numbers","text":"<p>BigDecimal mapping depends on precision:</p> Precision Parquet Type \u2264 9 int32 (DECIMAL) \u2264 18 int64 (DECIMAL) &gt; 18 binary (DECIMAL) or fixed_len_byte_array (DECIMAL)"},{"location":"advanced/data-types/#binary","title":"Binary","text":"<p>Carpet supports storing binary data using the <code>org.apache.parquet.io.api.Binary</code> class. This is useful for storing raw binary data that doesn't fit into other types. The <code>Binary</code> class provides methods for creating and manipulating binary data.</p> <p>Following record:</p> <pre><code>record SimpleRecord(long id, Binary data) { }\n</code></pre> <p>generates a Parquet schema with a <code>binary</code> type:</p> <pre><code>message SimpleRecord {\n    required int64 id;\n    optional binary data;\n}\n</code></pre>"},{"location":"advanced/data-types/#json-and-bson-types","title":"JSON and BSON types","text":"<p>Java doesn't have a native JSON or BSON type, but you can use <code>String</code> or <code>org.apache.parquet.io.api.Binary</code> to store JSON or BSON data.</p> <p>To configure it, you can use the <code>@ParquetJson</code> or <code>@ParquetBson</code> annotations to specify the logical type in Parquet schema.</p> <p>You can find more information about JSON and BSON in the Java Type Annotations section.</p>"},{"location":"advanced/data-types/#geospatial-types","title":"Geospatial Types","text":"<p>Carpet supports storing geospatial data using JTS (Java Topology Suite) Geometry objects or Binary representations with Well-Known Binary (WKB) format.</p>"},{"location":"advanced/data-types/#jts-geometry-support","title":"JTS Geometry Support","text":"<p>You can use JTS Geometry objects directly in your records. JTS Geometry objects must be annotated with <code>@ParquetGeometry</code> or <code>@ParquetGeography</code> to specify the type of geospatial data and its configuration.</p> <pre><code>record Location(String name, @ParquetGeometry Geometry geom) { }\n\nGeometryFactory factory = new GeometryFactory();\nPoint point = factory.createPoint(new Coordinate(1.0, 2.0));\nvar location = new Location(\"Office\", point);\n</code></pre> <p>Carpet will serialize the Geometry object (any JTS Geometry) to WKB format and store it in a Parquet <code>binary</code> field with the appropriate logical type.</p>"},{"location":"advanced/data-types/#binary-geometry-support","title":"Binary Geometry Support","text":"<p>You can also store geospatial data as Binary using WKB format created from other sources or libraries:</p> <pre><code>record GeospatialRecord(String id, @ParquetGeometry Binary geometry) { }\n\nWKBWriter writer = new WKBWriter();\nBinary wkb = Binary.fromConstantByteArray(writer.write(geometry));\nvar record = new GeospatialRecord(\"location1\", wkb);\n</code></pre>"},{"location":"advanced/data-types/#geometry-vs-geography","title":"Geometry vs Geography","text":"<ul> <li><code>@ParquetGeometry</code>: For planar/projected coordinate systems</li> <li><code>@ParquetGeography</code>: For spherical coordinate systems (latitude/longitude)</li> </ul> <p>Both annotations support additional configuration options like coordinate reference systems (CRS) and edge algorithms.</p> <p>You can find more information about geospatial annotations and their configuration options in the Java Type Annotations section.</p>"},{"location":"advanced/data-types/#nested-structures","title":"Nested Structures","text":"<p>Parquet supports nested structures, and Carpet can generate them using Java records and Collections.</p> <p>Carpet uses the following rules to generate nested structures:</p> <ol> <li>Types must be concrete and cannot be generic.</li> <li>Types cannot be recursive directly or indirectly.</li> </ol>"},{"location":"advanced/data-types/#nested-records","title":"Nested Records","text":"<p>Carpet supports nested records to create files with structured data. There is one exception: types can not be recursive directly nor indirectly.</p> <pre><code>record Address(String street, String zip, String city) { }\nrecord Job(String company, String position, int years) { }\nrecord Person(long id, Job job, Address address) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Person.class)) {\n    var president = new Person(1010101, new Job(\"USA\", POTUS, 3),\n        new Address(\"1600 Pennsylvania Av.\", \"20500\", \"Washington\"));\n    writer.write(president));\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Person {\n  required int64 id;\n  optional group job {\n    optional binary company (STRING);\n    optional binary position (STRING);\n    required int32 years;\n  }\n  optional group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre>"},{"location":"advanced/data-types/#collections","title":"Collections","text":"<p>Carpet supports nested collections to create files with structured data. Collection elements must be one of the supported types.</p> <pre><code>record Line(String sku, int quantity, double price){ }\nrecord Invoice(String id, double amount, double taxes, List&lt;Line&gt; lines) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Invoice.class)) {\n    var invoice = new Invoice(\"2023/211\", 2323.23, 232.32, List.of(\n        new Line(\"AAA\", 3, 500.0), new Line(\"BBB\", 1, 823.23)));\n    writer.write(invoice);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a collection to any type of Java <code>Collection</code> implementation. The only restriction is that the Collection type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#maps","title":"Maps","text":"<p>Carpet supports nested maps to create files with structured data. Map elements must be one of the supported types.</p> <pre><code>record State(double area, int population){ }\nrecord Country(String name, double area, Map&lt;String, State&gt; states) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Country.class)) {\n    var country = new Country(\"USA\", 9_833_520.0, Map.of(\n        \"Idaho\", new State(216_444.0, 1_975_000),\n        \"Texas\", new State(695_662.0, 29_145_505)));\n    writer.write(country);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Country {\n  optional binary name (STRING);\n  required double area;\n  optional group states (MAP) {\n    repeated group key_value {\n      required binary key (STRING);\n      optional group value {\n        required double area;\n        required int32 population;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a map to any type of Java <code>Map</code> implementation. The only restriction is that the Map type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#limitations","title":"Limitations","text":""},{"location":"advanced/data-types/#generic-types","title":"Generic Types","text":"<p>Records cannot have generic elements as Carpet needs concrete type information to create the Parquet schema. This includes generic records, collections, and maps.</p> <pre><code>// This will NOT work\nrecord WithGeneric&lt;T&gt;(String name, T child) { }\n\n// This works fine\nrecord WithList(String name, List&lt;String&gt; items) { }\nrecord WithMap(String name, Map&lt;String, Integer&gt; values) { }\n</code></pre> <p>If generic type is used, <code>RecordTypeConversionException</code> will be thrown.</p> <p>Collections and Maps with concrete types don't have this issue because Carpet knows their concrete type information at compile time.</p>"},{"location":"advanced/data-types/#recursive-types","title":"Recursive Types","text":"<p>Records cannot have direct or indirect recursive references:</p> <pre><code>// This will NOT work - direct recursion\nrecord Node(String id, Node next) { }\n\n// This will NOT work - indirect recursion\nrecord Child(String id, Parent parent) { }\nrecord Parent(String id, Child child) { }\n</code></pre>"},{"location":"advanced/input-output-files/","title":"Input and Output Files","text":"<p><code>parquet-java</code> defines <code>OutputFile</code> and <code>InputFile</code> interfaces to interact with files. Originally, it only provided <code>HadoopOutputFile</code> and <code>HadoopInputFile</code> implementations that were capable of working with Hadoop and local files.</p> <p>This required a Hadoop dependency to be included in the project. This is not ideal for projects that only need to work with local files, as it adds unnecessary complexity and size to the project. To address this, Parquet Java recently added <code>LocalOutputFile</code> and <code>LocalInputFile</code> implementations.</p> <p>Before these classes were created, Carpet provided a local file implementation with <code>FileSystemOutputFile</code> and <code>FileSystemInputFile</code>. You can use either implementation.</p>"},{"location":"advanced/java-type-annotations/","title":"Java Type Annotations","text":"<p>Carpet allows you to store <code>String</code>, Enum, or <code>org.apache.parquet.io.api.Binary</code> fields as the Parquet <code>BINARY</code> type with different logical types, such as String, Enum, JSON, or BSON. This is useful for embedding JSON, BSON documents, or any raw binary data directly into your Parquet files if no native type is available for your use case.</p> <p>You can use the <code>@ParquetString</code>, <code>@ParquetEnum</code>, <code>@ParquetJson</code>, or <code>@ParquetBson</code> annotations to configure the logical type in the Parquet schema. These annotations do not transform or convert the actual data. They simply specify how the data should be interpreted in the Parquet format. Carpet does not validate the content of the data, so you must ensure that the data you are writing is valid String, JSON, or BSON.</p> <p>These annotations can be applied to record components or collection elements (<code>List&lt;@ParquetBson Binary&gt; values</code>). The following sections describe how to use these annotations with different types.</p>"},{"location":"advanced/java-type-annotations/#parquetstring-annotation","title":"@ParquetString annotation","text":"<p>The <code>@ParquetString</code> annotation is used to specify that a field should be stored as a Parquet string type when the field type is an Enum or a Parquet Java <code>Binary</code> type.</p> <p>By default, Carpet converts <code>Binary</code> and Enum types to their corresponding Parquet types. However, for some use cases, you may want to store them as binary strings instead, overriding the default behavior.</p>"},{"location":"advanced/java-type-annotations/#with-binary-type","title":"With Binary type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetString Binary code) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary code (STRING);\n}\n</code></pre> <p>This is useful when the source of your information is a <code>Binary</code> type, but you still want to store it as a string in Parquet.</p>"},{"location":"advanced/java-type-annotations/#with-enum-type","title":"With Enum type","text":"<p>The following record:</p> <pre><code>enum Category { HIGH, MEDIUM, LOW }\n\nrecord Person(String name, @ParquetString Category category) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary category (STRING);\n}\n</code></pre> <p>You can work with enumerations while keeping their String representation in Parquet, without breaking contracts with other systems.</p>"},{"location":"advanced/java-type-annotations/#parquetenum-annotation","title":"@ParquetEnum annotation","text":"<p>The <code>@ParquetEnum</code> annotation is used to specify that a field should be stored as a Parquet enum type when the field type is a <code>String</code> or a Parquet Java <code>Binary</code> type.</p> <p>By default, Carpet converts <code>Binary</code> and <code>String</code> types to their corresponding Parquet types. However, for some use cases, you may want to store them as binary Enum instead, overriding the default behavior.</p>"},{"location":"advanced/java-type-annotations/#with-binary-type_1","title":"With Binary type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetEnum Binary code) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary code (ENUM);\n}\n</code></pre> <p>This is useful when the source of your information is a <code>Binary</code> type, but you still want to store it as an Enum in Parquet.</p>"},{"location":"advanced/java-type-annotations/#with-string-type","title":"With String type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetEnum String category) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary category (ENUM);\n}\n</code></pre> <p>You can work with Strings while keeping their Enum representation in Parquet, without breaking contracts with other systems.</p>"},{"location":"advanced/java-type-annotations/#parquetjson-annotation","title":"@ParquetJson annotation","text":"<p>Java does not have a native JSON type, but you can use <code>String</code> or <code>Binary</code> to store JSON data. The <code>@ParquetJson</code> annotation is used to specify that a field should be stored as a Parquet JSON type when the field type is a <code>String</code> or <code>Binary</code>.</p> <p>To store a field as JSON, annotate the record component with <code>@ParquetJson</code>. The data will be stored as Parquet <code>binary</code> with the <code>JSON</code> logical type.</p> <p>The following record:</p> <pre><code>record ProductEvent(long id, Instant timestamp, @ParquetJson String jsonData){}\n</code></pre> <p>generates a schema with a <code>binary</code> field annotated with the <code>JSON</code> logical type:</p> <pre><code>message ProductEvent {\n    required int64 id;\n    required int64 timestamp (TIMESTAMP(MILLIS,true));\n    optional binary jsonData (JSON);\n}\n</code></pre> <p><code>@ParquetJson</code> can also annotate the <code>Binary</code> class.</p>"},{"location":"advanced/java-type-annotations/#parquetbson-annotation","title":"@ParquetBson annotation","text":"<p>Similar to JSON, Java does not have a native BSON type, but you can use the <code>Binary</code> type to store BSON data. The <code>@ParquetBson</code> annotation is used to specify that a field should be stored as a Parquet BSON type when the field type is <code>Binary</code>.</p> <p>The following record:</p> <pre><code>record ProductEvent(long id, Instant timestamp, @ParquetBson Binary bsonData){}\n</code></pre> <p>generates a schema with a <code>binary</code> field annotated with the <code>BSON</code> logical type:</p> <pre><code>message ProductEvent {\n    required int64 id;\n    required int64 timestamp (TIMESTAMP(MILLIS,true));\n    optional binary bsonData (BSON);\n}\n</code></pre> <p>Carpet does not validate the content of the data, so you must ensure that the data you are writing is valid BSON.</p>"},{"location":"advanced/java-type-annotations/#bigdecimal-type","title":"BigDecimal type","text":"<p>The <code>BigDecimal</code> type is used to represent arbitrary-precision decimal numbers. In Parquet, <code>BigDecimal</code> can be represented by multiple physical Parquet types, all configured with the <code>DECIMAL</code> logical type and a specified precision and scale.</p>"},{"location":"advanced/java-type-annotations/#precisionscale-annotation","title":"@PrecisionScale annotation","text":"<p>The precision is the total number of digits, and the scale is the number of digits to the right of the decimal point.</p> <p>When writing a file, the precision and scale can be configured globally in the writer configuration or per record field using the <code>@PrecisionScale</code> annotation. Annotation configuration takes precedence over the writer configuration.</p> <p>The following record:</p> <pre><code>record Product(long id, @PrecisionScale(20, 4) BigDecimal price) {}\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Product {\n  required int64 id;\n  optional binary price (DECIMAL(20,4));\n}\n</code></pre> <p>When writing a file with a configured precision and scale, Carpet adapts the data to these specifications. If the data in the file has a different precision or scale, it will be converted to the specified precision and scale.</p> <p>When reading a file with a record field annotated with <code>@PrecisionScale</code>, Carpet does NOT validate the precision and scale of the data. It reads the data as <code>BigDecimal</code> using the precision and scale from the file. If the data in the file has a different precision or scale, Carpet will not throw an error or convert it. You must ensure that the data you are reading is valid for the specified precision and scale.</p>"},{"location":"advanced/java-type-annotations/#rounding-annotation","title":"@Rounding annotation","text":"<p>If scale adjustment is needed, you must configure the rounding mode to round the value to the specified scale.</p> <p>When writing a file, the rounding mode can be configured globally in the writer configuration or per record field using the <code>@Rounding</code> annotation. Annotation configuration takes precedence over the writer configuration.</p> <p>The <code>@Rounding</code> annotation requires a <code>RoundingMode</code> enum parameter, which is used to round <code>BigDecimal</code> values in the Java API. This annotation does not modify the generated Parquet schema but configures the rounding mode for <code>BigDecimal</code> values.</p> <pre><code>record Product(\n    long id,\n    @PrecisionScale(20, 4) @Rounding(RoundingMode.HALF_UP) BigDecimal price) {\n}\n</code></pre> <p>If the rounding mode is not specified via annotation or writer configuration, the default is <code>RoundingMode.UNNECESSARY</code>. This means an exception will be thrown if rounding is necessary, which is useful to ensure data integrity if no changes are expected during conversion.</p> <p><code>@PrecisionScale</code> and <code>@Rounding</code> annotations can be used together or separately, depending on your use case and how you want to configure the precision and scale of <code>BigDecimal</code> values in your Parquet files.</p>"},{"location":"advanced/java-type-annotations/#geospatial-type-annotations","title":"Geospatial Type Annotations","text":"<p>Carpet supports storing geospatial data using specialized annotations that define how geometry and geography data should be stored in Parquet files. These annotations can be applied to <code>org.locationtech.jts.geom.Geometry</code> fields, or Parquet <code>Binary</code> fields containing Well-Known Binary (WKB) geometry data.</p>"},{"location":"advanced/java-type-annotations/#parquetgeometry-annotation","title":"@ParquetGeometry annotation","text":"<p>The <code>@ParquetGeometry</code> annotation is used to specify that a field should be stored as a Parquet geometry type for planar/projected coordinate systems. This annotation is suitable for geometry data that uses projected coordinate reference systems where calculations are performed on a flat plane.</p>"},{"location":"advanced/java-type-annotations/#with-jts-geometry","title":"With JTS Geometry","text":"<p>The following record uses JTS Geometry objects directly:</p> <pre><code>record Location(String name, @ParquetGeometry Geometry geom) { }\n</code></pre> <p>This will be converted to a Parquet schema with a <code>binary</code> field annotated with the <code>GEOMETRY</code> logical type, and Carpet will serialize the Geometry object (any JTS Geometry) to WKB format:</p> <pre><code>message Location {\n  optional binary name (STRING);\n  optional binary geom (GEOMETRY);\n}\n</code></pre>"},{"location":"advanced/java-type-annotations/#with-binary-wkb-format","title":"With Binary (WKB format)","text":"<p>You can also store geometry data as Binary using Well-Known Binary format:</p> <pre><code>record LocationBinary(String name, @ParquetGeometry Binary geom) { }\n</code></pre>"},{"location":"advanced/java-type-annotations/#with-coordinate-reference-system","title":"With Coordinate Reference System","text":"<p>You can specify a coordinate reference system by providing a CRS identifier:</p> <pre><code>record LocationWithCRS(String name, @ParquetGeometry(\"EPSG:3857\") Geometry geom) { }\n</code></pre> <p>This information will be included in the Parquet schema metadata to inform readers about the CRS used.</p>"},{"location":"advanced/java-type-annotations/#parquetgeography-annotation","title":"@ParquetGeography annotation","text":"<p>The <code>@ParquetGeography</code> annotation is used to specify that a field should be stored as a Parquet geography type for spherical coordinate systems. This annotation is suitable for geographic data that uses latitude/longitude coordinates on the Earth's surface.</p>"},{"location":"advanced/java-type-annotations/#with-jts-geometry_1","title":"With JTS Geometry","text":"<p>The following record stores geographic data:</p> <pre><code>record WorldLocation(String name, @ParquetGeography Geometry location) { }\n</code></pre> <p>This will be converted to a Parquet schema with a <code>binary</code> field annotated with the <code>GEOGRAPHY</code> logical type, and Carpet will serialize the Geometry object (any JTS Geometry) to WKB format:</p> <pre><code>message WorldLocation {\n  optional binary name (STRING);\n  optional binary location (GEOGRAPHY);\n}\n</code></pre>"},{"location":"advanced/java-type-annotations/#with-binary-wkb-format_1","title":"With Binary (WKB format)","text":"<p>You can also store geography data as Binary using Well-Known Binary format:</p> <pre><code>record WorldLocationBinary(String name, @ParquetGeography Binary location) { }\n</code></pre>"},{"location":"advanced/java-type-annotations/#with-configuration-options","title":"With Configuration Options","text":"<p>The <code>@ParquetGeography</code> annotation supports additional configuration for precise geographic calculations:</p> <pre><code>record PreciseLocation(\n    String name,\n    @ParquetGeography(crs = \"EPSG:4326\", algorithm = EdgeAlgorithm.VINCENTY) Geometry location\n) { }\n</code></pre> <p>This information will be included in the Parquet schema metadata to inform readers about the CRS and edge calculation algorithm used.</p>"},{"location":"advanced/java-type-annotations/#coordinate-reference-system-crs","title":"Coordinate Reference System (CRS)","text":"<p>The <code>crs</code> parameter specifies the coordinate reference system, for example:</p> <ul> <li><code>\"EPSG:4326\"</code> - WGS 84 (World Geodetic System 1984) - most common for GPS coordinates</li> <li><code>\"EPSG:3857\"</code> - Web Mercator projection - commonly used in web mapping</li> <li><code>\"OGC:CRS84\"</code> - WGS 84 longitude/latitude order</li> <li><code>\"\"</code> (empty) - uses default CRS</li> </ul>"},{"location":"advanced/java-type-annotations/#edge-interpolation-algorithms","title":"Edge Interpolation Algorithms","text":"<p>The <code>algorithm</code> parameter determines how edges between geographic points are calculated:</p> <ul> <li><code>EdgeAlgorithm.SPHERICAL</code> - Fast spherical interpolation, assumes Earth is a perfect sphere</li> <li><code>EdgeAlgorithm.VINCENTY</code> - High accuracy geodesic calculations on ellipsoid</li> <li><code>EdgeAlgorithm.THOMAS</code> - Optimized version of Vincenty's formula, good balance of accuracy and performance</li> <li><code>EdgeAlgorithm.ANDOYER</code> - Fast approximation for short distances</li> <li><code>EdgeAlgorithm.KARNEY</code> - Most accurate geodesic calculations, computationally intensive</li> </ul>"},{"location":"advanced/java-type-annotations/#collection-support","title":"Collection Support","text":"<p>Geospatial annotations can be applied to collection elements:</p> <pre><code>record MultiLocationRecord(\n    String name,\n    List&lt;@ParquetGeography Geometry&gt; locations,\n    Map&lt;String, @ParquetGeometry Binary&gt; regions\n) { }\n</code></pre>"},{"location":"advanced/low-level-parquet/","title":"Low level Parquet classes","text":"<p>Carpet is built on top of parquet-java library and supports creating native library <code>ParquetWriter</code> and <code>ParquetReader</code> classes, and use it with third party libraries that work with Parquet classes.</p>"},{"location":"advanced/low-level-parquet/#parquetwriter","title":"ParquetWriter","text":"<pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\nPath path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nOutputFile outputFile = HadoopOutputFile.fromPath(path, new Configuration());\ntry (ParquetWriter&lt;MyRecord&gt; writer = CarpetParquetWriter.builder(outputFile, MyRecord.class)\n        .withWriteMode(Mode.OVERWRITE)\n        .withCompressionCodec(CompressionCodecName.GZIP)\n        .withPageRowCountLimit(100_000)\n        .withBloomFilterEnabled(\"name\", true)\n        .build()) {\n\n    otherLibraryIntegrationWrite(writer, data);\n}\n</code></pre>"},{"location":"advanced/low-level-parquet/#parquetreader","title":"ParquetReader","text":"<pre><code>Path path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nInputFile inputFile = new HadoopInputFile(path, new Configuration());\ntry (ParquetReader&lt;MyRecord&gt; reader = CarpetParquetReader.builder(inputFile, MyRecord.class).build()) {\n    var data = otherLibraryIntegrationRead(reader);\n    // process data\n}\n</code></pre>"},{"location":"advanced/nullability/","title":"Nullability","text":"<p>Parquet supports to configure not null columns in the schema. Carpet, writing the schema, respects Java primitives' nullability:</p> <p>This record:</p> <pre><code>record MyRecord(long id, String name, int size, double value){ }\n</code></pre> <p>generates this schema with primitive types as <code>required</code>:</p> <pre><code>message MyRecord {\n  required int64 id;\n  optional binary name (STRING);\n  required int32 size;\n  required double value;\n}\n</code></pre> <p>while this record:</p> <pre><code>record MyRecord(Long id, String name, Integer size, Double value) { }\n</code></pre> <p>generates this schema with all numeric values as <code>optional</code>:</p> <pre><code>message MyRecord {\n  optional int64 id;\n  optional binary name (STRING);\n  optional int32 size;\n  optional double value;\n}\n</code></pre> <p>String, List or Map types are objects and can be nullable. To generate a schema where an object reference field is created as <code>required</code> you must annotate the field with <code>@NotNull</code> annotation.</p> <pre><code>record MyRecord(@NotNull String id, @NotNull String name, @NotNull Address address){ }\n</code></pre> <p>generates this schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  required group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre> <p>The <code>@NotNull</code> annotation is not part of the Java standard library and Carpet provides one implementation. You can use any library that provides this type of annotation, such as <code>javax.validation.constraints.NotNull</code> or <code>jakarta.annotation.Nonnull</code>. Carpet inspects fields annotation looking by the name of the annotation not the complete type.</p>"},{"location":"advanced/projections/","title":"Projections","text":"<p>One of key features of Parquet is that you can save a lot of I/O and CPU if you read only a subset of available columns in a file.</p> <p>Given a parquet file, you can read a subset of columns just using a Record with needed columns.</p> <p>For example, from a file with this schema, you can read just id, sku, and quantity fields:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  required double amount;\n  required double taxes;\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>defining this records:</p> <pre><code>record LineRead(String sku, int quantity) { }\n\nrecord InvoiceRead(String id, List&lt;LineRead&gt; lines) { }\n\nList&lt;InvoiceRead&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), InvoiceRead.class).toList();\n</code></pre> <p>Parquet will read and parse only pages with id, sku, and quantity columns, skipping the rest of the file.</p>"},{"location":"advanced/schema-mismatch/","title":"Read Schema mismatch","text":"<p>How does Carpet behave when the schema does not exactly match records types?</p>"},{"location":"advanced/schema-mismatch/#nullable-column-mapped-to-primitive-type","title":"Nullable column mapped to primitive type","text":"<p>By default Carpet doesn't fail when a column is defined as <code>optional</code> but the record field is primitive.</p> <p>This parquet schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  optional int32 age;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>When a null value appears in a file, the field is filled with the default value of the primitive (0, 0.0 or false).</p> <p>If you want to ensure that the application fails if an optional column is mapped to a primitive field, you must enable the flag <code>FailOnNullForPrimitives</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnNullForPrimitives(true)\n    .toList();\n</code></pre> <p>By default, <code>FailOnNullForPrimitives</code> value is false.</p>"},{"location":"advanced/schema-mismatch/#missing-fields","title":"Missing fields","text":"<p>When parquet file schema doesn't match with existing record fields, Carpet throws an exception.</p> <p>This schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n}\n</code></pre> <p>is not compatible with this record because it contains an additional <code>int age</code> field:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>If for some reason you are forced to read the file with an incompatible record, you can disable the schema compatibility check with flag <code>FailOnMissingColumn</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnMissingColumn(false)\n    .toList();\n</code></pre> <p>Carpet will skip the schema verification and fill the value with <code>null</code> in case of Objects or the default value of primitives (0, 0.0 or false).</p> <p>By default, <code>FailOnMissingColumn</code> value is true.</p> <p>If a column that exists in the file is not present in the record, Carpet will ignore it and will not throw an exception because it's considered a projection.</p>"},{"location":"advanced/schema-mismatch/#narrowing-numeric-values","title":"Narrowing numeric values","text":"<p>By default Carpet converts between numeric types:</p> <ul> <li>Any integer type can be converted to another integer type of different size: byte &lt;-&gt; short &lt;-&gt; int &lt;-&gt; long.</li> <li>Any decimal type can be converted to another decimal type of different size: float &lt;-&gt; double</li> </ul> <p>This schema</p> <pre><code>message MyRecord {\n  required int64 id;\n  required double value;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(int id, float value) { }\n</code></pre> <p>Carpet will cast numeric types using Narrowing Primitive Conversion rules from Java.</p> <p>If you want to ensure that the application fails if a type is converted to a narrow value, you can enable the flag <code>FailNarrowingPrimitiveConversion</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailNarrowingPrimitiveConversion(true)\n    .toList();\n</code></pre> <p>By default, <code>FailNarrowingPrimitiveConversion</code> value is false.</p>"},{"location":"advanced/write-model/","title":"Write Model","text":"<p>The Write Model feature in Carpet allows you to customize how your Java objects are written to Parquet files. While Carpet can automatically map Java Records to Parquet schemas using reflection, the Write Model provides fine-grained control over field mapping, data types, and serialization behavior.</p>"},{"location":"advanced/write-model/#what-is-a-write-model","title":"What is a Write Model?","text":"<p>A Write Model is a programmatic specification that defines:</p> <ul> <li>Which fields from your Java objects should be written to the Parquet file</li> <li>How those fields should be named in the Parquet schema</li> <li>What Parquet data types should be used for each field</li> <li>How to extract values from your objects (accessor functions)</li> </ul> <p>This is particularly useful when:</p> <ul> <li>You want to write only specific fields from your objects</li> <li>You need to rename fields in the Parquet file</li> <li>You're working with classes that aren't records</li> <li>You want to customize data types or logical types</li> <li>You need to write data in a specific format for compatibility</li> </ul>"},{"location":"advanced/write-model/#understanding-parquet-context","title":"Understanding Parquet Context","text":"<p>Parquet is a columnar storage format that requires a predefined schema. When writing data to Parquet:</p> <ol> <li>Schema Definition: The structure must be known before writing any data</li> <li>Data Types: Each column has a specific primitive type (INT32, INT64, BINARY, etc.)</li> <li>Logical Types: Additional metadata that defines how to interpret primitive types</li> <li>Repetition: Fields can be required, optional, or repeated (for collections)</li> </ol> <p>Carpet's Write Model bridges the gap between your Java objects and Parquet's requirements, giving you control over this mapping process.</p>"},{"location":"advanced/write-model/#basic-usage","title":"Basic Usage","text":""},{"location":"advanced/write-model/#creating-a-write-model","title":"Creating a Write Model","text":"<p>Use the <code>writeRecordModel()</code> factory method to create a Write Model for your class:</p> <pre><code>import static com.jerolba.carpet.model.FieldTypes.*;\n\n// Define your data class\nrecord Person(String name, int age, String email) {}\n\n// Create a Write Model\nvar writeModel = writeRecordModel(Person.class)\n    .withField(\"person_name\", STRING, Person::name)\n    .withField(\"person_age\", INTEGER, Person::age)\n    .withField(\"email_address\", STRING, Person::email);\n</code></pre>"},{"location":"advanced/write-model/#using-the-write-model","title":"Using the Write Model","text":"<p>Apply the Write Model when creating a Parquet writer:</p> <pre><code>// Using CarpetWriter\ntry (CarpetWriter&lt;Person&gt; writer = new CarpetWriter.Builder&lt;&gt;(outputStream, Person.class)\n        .withWriteRecordModel(writeModel)\n        .build()) {\n    writer.write(List.of(\n        new Person(\"Alice\", 30, \"alice@example.com\"),\n        new Person(\"Bob\", 25, \"bob@example.com\")\n    ));\n}\n\n// Using CarpetParquetWriter\ntry (ParquetWriter&lt;Person&gt; writer = CarpetParquetWriter.builder(outputFile, Person.class)\n        .withWriteRecordModel(writeModel)\n        .build()) {\n    writer.write(new Person(\"Charlie\", 35, \"charlie@example.com\"));\n}\n</code></pre>"},{"location":"advanced/write-model/#field-mapping","title":"Field Mapping","text":""},{"location":"advanced/write-model/#basic-field-types","title":"Basic Field Types","text":"<p>Map fields using primitive type accessors and field type constants:</p> <pre><code>record Product(int id, String name, Double price, boolean available) {}\n\nvar writeModel = writeRecordModel(Product.class)\n    .withField(\"product_id\", Product::id)           // Primitive int\n    .withField(\"name\", STRING, Product::name)       // String with type\n    .withField(\"price\", DOUBLE, Product::price)     // Double object\n    .withField(\"in_stock\", Product::available);     // Primitive boolean\n</code></pre>"},{"location":"advanced/write-model/#primitive-type-methods","title":"Primitive Type Methods","text":"<p>For primitive types, you can use specialized accessor methods that avoid boxing and specify the field type implicitly:</p> <pre><code>record Metrics(int count, long total, float average, double ratio,\n               short level, byte flag, boolean active) {}\n\nvar writeModel = writeRecordModel(Metrics.class)\n    .withField(\"count\", Metrics::count)         // ToIntFunction\n    .withField(\"total\", Metrics::total)         // ToLongFunction\n    .withField(\"average\", Metrics::average)     // ToFloatFunction\n    .withField(\"ratio\", Metrics::ratio)         // ToDoubleFunction\n    .withField(\"level\", Metrics::level)         // ToShortFunction\n    .withField(\"flag\", Metrics::flag)           // ToByteFunction\n    .withField(\"active\", Metrics::active);      // ToBooleanFunction\n</code></pre>"},{"location":"advanced/write-model/#object-field-types","title":"Object Field Types","text":"<p>For object types, you need to specify both the field type and accessor function:</p> <pre><code>record Order(UUID id, String customerName, BigDecimal amount,\n             LocalDate orderDate, LocalDateTime createdAt) {}\n\nvar writeModel = writeRecordModel(Order.class)\n    .withField(\"order_id\", UUID, Order::id)\n    .withField(\"customer\", STRING, Order::customerName)\n    .withField(\"amount\", BIG_DECIMAL, Order::amount)\n    .withField(\"order_date\", LOCAL_DATE, Order::orderDate)\n    .withField(\"created_at\", LOCAL_DATE_TIME, Order::createdAt);\n</code></pre>"},{"location":"advanced/write-model/#working-with-collections","title":"Working with Collections","text":""},{"location":"advanced/write-model/#lists","title":"Lists","text":"<p>Use <code>LIST.ofType()</code> to define list fields:</p> <pre><code>record ShoppingCart(String userId, List&lt;String&gt; productIds, List&lt;Integer&gt; quantities) {}\n\nvar writeModel = writeRecordModel(ShoppingCart.class)\n    .withField(\"user_id\", STRING, ShoppingCart::userId)\n    .withField(\"products\", LIST.ofType(STRING), ShoppingCart::productIds)\n    .withField(\"quantities\", LIST.ofType(INTEGER), ShoppingCart::quantities);\n</code></pre>"},{"location":"advanced/write-model/#sets","title":"Sets","text":"<p>Sets are handled similarly to lists and are mapped to Parquet with the same type that lists use:</p> <pre><code>record UserProfile(String username, Set&lt;String&gt; tags, Set&lt;Category&gt; categories) {}\n\nvar writeModel = writeRecordModel(UserProfile.class)\n    .withField(\"username\", STRING, UserProfile::username)\n    .withField(\"tags\", SET.ofType(STRING), UserProfile::tags)\n    .withField(\"categories\", SET.ofType(ENUM.ofType(Category.class)), UserProfile::categories);\n</code></pre>"},{"location":"advanced/write-model/#maps","title":"Maps","text":"<p>Use <code>MAP.ofTypes()</code> for key-value mappings. Maps require both key and value types:</p> <pre><code>record Configuration(String appName, Map&lt;String, String&gt; settings,\n                    Map&lt;String, Integer&gt; counters) {}\n\nvar writeModel = writeRecordModel(Configuration.class)\n    .withField(\"app_name\", STRING, Configuration::appName)\n    .withField(\"settings\", MAP.ofTypes(STRING, STRING), Configuration::settings)\n    .withField(\"counters\", MAP.ofTypes(STRING, INTEGER), Configuration::counters);\n</code></pre>"},{"location":"advanced/write-model/#working-with-nested-records","title":"Working with Nested Records","text":"<p>Write Models support nested structures by composing multiple <code>writeRecordModel</code> definitions. This allows you to define complex hierarchical data structures where one record contains another record as a field.</p>"},{"location":"advanced/write-model/#basic-nested-records","title":"Basic Nested Records","text":"<p>Define nested structures by using a <code>writeRecordModel</code> as a field type:</p> <pre><code>record Address(String street, String city, String zipCode) {}\nrecord Person(String name, int age, Address address) {}\n\n// Define the nested record model first\nvar addressModel = writeRecordModel(Address.class)\n    .withField(\"street\", STRING, Address::street)\n    .withField(\"city\", STRING, Address::city)\n    .withField(\"zip_code\", STRING, Address::zipCode);\n\n// Use the nested model in the parent record\nvar personModel = writeRecordModel(Person.class)\n    .withField(\"full_name\", STRING, Person::name)\n    .withField(\"age\", INTEGER, Person::age)\n    .withField(\"address\", addressModel, Person::address);\n</code></pre>"},{"location":"advanced/write-model/#inline-nested-models","title":"Inline Nested Models","text":"<p>You can also define nested models inline for simpler cases:</p> <pre><code>record Department(String name, String code) {}\nrecord Employee(String name, Department department) {}\n\nvar employeeModel = writeRecordModel(Employee.class)\n    .withField(\"employee_name\", STRING, Employee::name)\n    .withField(\"dept\", writeRecordModel(Department.class)\n        .withField(\"dept_name\", STRING, Department::name)\n        .withField(\"dept_code\", STRING, Department::code), Employee::department);\n</code></pre>"},{"location":"advanced/write-model/#multiple-level-nesting","title":"Multiple Level Nesting","text":"<p>Write Models support arbitrary levels of nesting:</p> <pre><code>record Country(String name, String code) {}\nrecord State(String name, Country country) {}\nrecord City(String name, State state) {}\nrecord Address(String street, City city) {}\n\n// Build models from innermost to outermost\nvar countryModel = writeRecordModel(Country.class)\n    .withField(\"name\", STRING, Country::name)\n    .withField(\"code\", STRING, Country::code);\n\nvar stateModel = writeRecordModel(State.class)\n    .withField(\"name\", STRING, State::name)\n    .withField(\"country\", countryModel, State::country);\n\nvar cityModel = writeRecordModel(City.class)\n    .withField(\"name\", STRING, City::name)\n    .withField(\"state\", stateModel, City::state);\n\nvar addressModel = writeRecordModel(Address.class)\n    .withField(\"street\", STRING, Address::street)\n    .withField(\"city\", cityModel, Address::city);\n</code></pre>"},{"location":"advanced/write-model/#handling-null-nested-objects","title":"Handling Null Nested Objects","text":"<p>Nested records can be null, and the Write Model handles this gracefully:</p> <pre><code>record ContactInfo(String email, String phone) {}\nrecord Customer(String name, ContactInfo contact) {}\n\nvar customerModel = writeRecordModel(Customer.class)\n    .withField(\"customer_name\", STRING, Customer::name)\n    .withField(\"contact\", writeRecordModel(ContactInfo.class)\n        .withField(\"email\", STRING, ContactInfo::email)\n        .withField(\"phone\", STRING, ContactInfo::phone), Customer::contact);\n\n// Usage with null nested object\nvar customer1 = new Customer(\"John Doe\", new ContactInfo(\"john@example.com\", \"555-0123\"));\nvar customer2 = new Customer(\"Jane Smith\", null); // Null contact info\n</code></pre>"},{"location":"advanced/write-model/#nested-records-with-collections","title":"Nested Records with Collections","text":"<p>Combine nested records with collections for complex data structures:</p> <pre><code>record Item(String name, BigDecimal price) {}\nrecord Order(String orderId, List&lt;Item&gt; items, Address shippingAddress) {}\n\nvar itemModel = writeRecordModel(Item.class)\n    .withField(\"product_name\", STRING, Item::name)\n    .withField(\"unit_price\", BIG_DECIMAL.withPrecisionScale(10, 2), Item::price);\n\nvar orderModel = writeRecordModel(Order.class)\n    .withField(\"order_id\", STRING, Order::orderId)\n    .withField(\"items\", LIST.ofType(itemModel), Order::items)\n    .withField(\"shipping_address\", addressModel, Order::shippingAddress);\n</code></pre> <p>Nested Write Model can also be used as keys or values in maps.</p>"},{"location":"advanced/write-model/#advanced-field-types","title":"Advanced Field Types","text":""},{"location":"advanced/write-model/#enums","title":"Enums","text":"<p>Configure how enums are stored in Parquet:</p> <pre><code>enum Priority { LOW, MEDIUM, HIGH }\n\nrecord Task(String title, Priority priority) {}\n\n// Store as enum logical type (default)\nvar writeModel1 = writeRecordModel(Task.class)\n    .withField(\"title\", STRING, Task::title)\n    .withField(\"priority\", ENUM.ofType(Priority.class), Task::priority);\n\n// Store as string logical type\nvar writeModel2 = writeRecordModel(Task.class)\n    .withField(\"title\", STRING, Task::title)\n    .withField(\"priority\", ENUM.ofType(Priority.class).asString(), Task::priority);\n</code></pre>"},{"location":"advanced/write-model/#bigdecimal-with-precision","title":"BigDecimal with Precision","text":"<p>Configure decimal precision and scale:</p> <pre><code>record FinancialRecord(String account, BigDecimal balance) {}\n\nvar writeModel = writeRecordModel(FinancialRecord.class)\n    .withField(\"account\", STRING, FinancialRecord::account)\n    .withField(\"balance\", BIG_DECIMAL.withPrecisionScale(10, 2),\n               FinancialRecord::balance);\n</code></pre>"},{"location":"advanced/write-model/#date-and-time-types","title":"Date and Time Types","text":"<p>Carpet supports all Java time types with proper Parquet logical type mappings:</p> <pre><code>record EventRecord(\n    String eventId,\n    LocalDate eventDate,        // Date only (year-month-day)\n    LocalTime eventTime,        // Time only (hour-minute-second-nano)\n    LocalDateTime timestamp,    // Date and time without timezone\n    Instant utcTimestamp        // UTC timestamp with timezone\n) {}\n\nvar writeModel = writeRecordModel(EventRecord.class)\n    .withField(\"event_id\", STRING, EventRecord::eventId)\n    .withField(\"event_date\", LOCAL_DATE, EventRecord::eventDate)\n    .withField(\"event_time\", LOCAL_TIME, EventRecord::eventTime)\n    .withField(\"timestamp\", LOCAL_DATE_TIME, EventRecord::timestamp)\n    .withField(\"utc_timestamp\", INSTANT, EventRecord::utcTimestamp);\n</code></pre> <p>The precision of time fields are globally configured in the CarpetWriter builder via <code>withDefaultTimeUnit</code> configuration method.</p>"},{"location":"advanced/write-model/#geospatial-data","title":"Geospatial Data","text":"<p>Handle geometric data with specific coordinate systems and geospatial types:</p> <pre><code>import org.locationtech.jts.geom.Geometry;\n\nrecord Location(String name, Geometry geometry) {}\n\nvar geometryModel = writeRecordModel(Location.class)\n    .withField(\"name\", STRING, Location::name)\n    .withField(\"geometry\", GEOMETRY.asParquetGeometry(\"EPSG:4326\"),\n               Location::geometry);\n\nvar geographyModel = writeRecordModel(Location.class)\n    .withField(\"name\", STRING, Location::name)\n    .withField(\"geometry\", GEOMETRY.asParquetGeography  (\"OGC:CRS84\", EdgeInterpolationAlgorithm.SPHERICAL),\n               Location::geometry);\n</code></pre>"},{"location":"advanced/write-model/#json-bson-and-binary-data","title":"JSON, BSON and Binary Data","text":"<p>Handle JSON strings, BSON and Binary data with logical types:</p> <pre><code>record Document(String title, String jsonData, Binary bsonData, Binary content) {}\n\nvar writeModel = writeRecordModel(Document.class)\n    .withField(\"title\", STRING, Document::title)\n    .withField(\"json_data\", STRING.asJson(), Document::jsonData)\n    .withField(\"bson_data\", BINARY.asBson(), Document::bsonData)\n    .withField(\"content\", BINARY, Document::content);\n</code></pre>"},{"location":"advanced/write-model/#custom-field-extraction","title":"Custom Field Extraction","text":"<p>You don't need to define new record classes for every use case. You can create custom fields or redefine existing ones using accessor functions.</p>"},{"location":"advanced/write-model/#computed-fields","title":"Computed Fields","text":"<p>Create fields based on calculations or transformations:</p> <pre><code>record Employee(String firstName, String lastName, LocalDate birthDate) {}\n\nvar now = LocalDate.now();\nvar writeModel = writeRecordModel(Employee.class)\n    .withField(\"first_name\", STRING, Employee::firstName)\n    .withField(\"last_name\", STRING, Employee::lastName)\n    .withField(\"full_name\", STRING, emp -&gt; emp.firstName() + \" \" + emp.lastName())\n    .withField(\"birth_date\", LOCAL_DATE, Employee::birthDate)\n    .withField(\"age_years\", INTEGER, emp -&gt;\n        Period.between(emp.birthDate(), now).getYears());\n</code></pre>"},{"location":"advanced/write-model/#conditional-logic","title":"Conditional Logic","text":"<p>Apply business logic during field extraction:</p> <pre><code>record Order(String id, double amount, String status) {}\n\nvar writeModel = writeRecordModel(Order.class)\n    .withField(\"order_id\", STRING, Order::id)\n    .withField(\"amount\", DOUBLE, Order::amount)\n    .withField(\"status\", STRING, Order::status)\n    .withField(\"is_large_order\", BOOLEAN, order -&gt; order.amount() &gt; 1000.0)\n    .withField(\"amount_category\", STRING, order -&gt; {\n        if (order.amount() &lt; 100) return \"SMALL\";\n        if (order.amount() &lt; 1000) return \"MEDIUM\";\n        return \"LARGE\";\n    });\n</code></pre>"},{"location":"advanced/write-model/#dynamic-fields-creation","title":"Dynamic fields creation","text":"<p>You can create fields dynamically based on runtime conditions or configurations:</p> <p>Following example creates fields based on a list of valid item codes:</p> <pre><code>record Item(String code, double value, String meta) {}\nrecord Order(UUID orderId, Map&lt;String, Item&gt; items) {}\n\nList&lt;String&gt; validCodes = List.of(\"a\", \"b\", \"c\", \"d\"); // Could be loaded from config\nvar writeModel = writeRecordModel(Order.class)\n    .withField(\"order_id\", UUID, Order::orderId);\nfor (var code : validCodes) {\n    writeModel.withField(\"item_\" + code + \"_value\", DOUBLE, order -&gt; order.items().get(code).value());\n    writeModel.withField(\"item_\" + code + \"_meta\", STRING, order -&gt; order.items().get(code).meta());\n}\n</code></pre>"},{"location":"advanced/write-model/#field-type-customization","title":"Field Type Customization","text":""},{"location":"advanced/write-model/#nullable-vs-not-null-fields","title":"Nullable vs Not-Null Fields","text":"<p>Control field nullability:</p> <pre><code>record Data(String required, String optional) {}\n\nvar writeModel = writeRecordModel(Data.class)\n    .withField(\"required_field\", STRING.notNull(), Data::required)\n    .withField(\"optional_field\", STRING, Data::optional);\n</code></pre>"},{"location":"advanced/write-model/#logical-types-for-binary-fields","title":"Logical Types for Binary Fields","text":"<p>Customize how binary data is written in the schema:</p> <pre><code>record BinaryData(Binary jsonData, Binary enumData, Binary stringData) {}\n\nvar writeModel = writeRecordModel(BinaryData.class)\n    .withField(\"json\", BINARY.asJson(), BinaryData::jsonData)\n    .withField(\"enum\", BINARY.asEnum(), BinaryData::enumData)\n    .withField(\"string\", BINARY.asString(), BinaryData::stringData);\n</code></pre>"},{"location":"advanced/write-model/#working-with-non-record-classes","title":"Working with Non-Record Classes","text":"<p>Write Models work with any Java class, not just records:</p> <pre><code>public class LegacyData {\n    private String id;\n    private int value;\n    private List&lt;String&gt; tags;\n\n    // constructors, getters, setters...\n    public String getId() { return id; }\n    public int getValue() { return value; }\n    public List&lt;String&gt; getTags() { return tags; }\n}\n\nvar writeModel = writeRecordModel(LegacyData.class)\n    .withField(\"identifier\", STRING, LegacyData::getId)\n    .withField(\"value\", INTEGER, LegacyData::getValue)\n    .withField(\"tags\", LIST.ofType(STRING), LegacyData::getTags);\n</code></pre>"},{"location":"advanced/write-model/#dynamic-model-creation","title":"Dynamic Model Creation","text":"<p>You can build Write Models dynamically at runtime based on configuration or metadata. This is particularly useful for:</p> <ul> <li>Creating flexible data export systems</li> <li>Adapting to changing schemas without code changes</li> <li>Creating generic data processing utilities</li> </ul> <pre><code>// Version-aware model creation\npublic static WriteRecordModelType&lt;Foo&gt; createVersionedModel(int schemaVersion) {\n\n    var modelBuilder = writeRecordModel(Foo.class);\n\n    // Base fields available in all versions\n    modelBuilder\n        .withField(\"id\", STRING, Foo::id)\n        .withField(\"name\", STRING, Foo::name);\n\n    // Version-specific fields\n    if (schemaVersion &gt;= 2) {\n        modelBuilder.withField(\"email\", STRING, Foo::email);\n    }\n\n    if (schemaVersion &gt;= 3) {\n        modelBuilder.withField(\"created_at\", INSTANT, Foo::createdAt);\n    }\n    return modelBuilder;\n}\n</code></pre>"},{"location":"advanced/write-model/#best-practices","title":"Best Practices","text":""},{"location":"advanced/write-model/#when-to-use-write-models","title":"When to Use Write Models","text":"<p>Use Write Models when:</p> <ul> <li>Working with legacy classes that aren't records</li> <li>Need to rename fields in the Parquet output</li> <li>Want to write only specific fields from your objects</li> <li>Need to compute derived fields</li> <li>Require specific data type control</li> <li>Working with runtime-determined schemas</li> </ul> <p>Use automatic mapping when:</p> <ul> <li>Java records map directly to desired Parquet schema</li> <li>Field names and types are already correct</li> <li>Simple, straightforward data serialization is needed</li> </ul>"},{"location":"advanced/write-model/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Write Models have no performance overhead compared to automatic mapping</li> <li>Accessor functions are called once per field per instance</li> <li>Complex computations in accessors can impact performance</li> <li>Consider caching expensive calculations</li> </ul>"},{"location":"advanced/write-model/#error-handling","title":"Error Handling","text":"<p>Write Models doesn't validate that field function match with the specified field type. Following code will throw a ClassCastException at runtime trying to write the file:</p> <pre><code>// This will throw an exception - mismatched class types\nvar invalidModel = writeRecordModel(Person.class)\n    .withField(\"name\", INTEGER, Person::name); // String -&gt; INTEGER mismatch\n</code></pre> <p>Accessor functions should handle null values appropriately:</p> <pre><code>var writeModel = writeRecordModel(Person.class)\n    .withField(\"name_length\", INTEGER, person -&gt;\n        person.name() != null ? person.name().length() : 0);\n</code></pre>"},{"location":"advanced/write-model/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example showing various Write Model features:</p> <pre><code>import static com.jerolba.carpet.model.FieldTypes.*;\n\nenum OrderStatus { PENDING, CONFIRMED, SHIPPED, DELIVERED }\n\nrecord Item(String sku, int quantity, BigDecimal price) {}\n\nrecord CustomerOrder(\n    UUID orderId,\n    String customerEmail,\n    List&lt;Item&gt; items,\n    OrderStatus status,\n    LocalDateTime createdAt,\n    Map&lt;String, String&gt; metadata\n) {}\n\n// Create a comprehensive Write Model\nvar writeModel = writeRecordModel(CustomerOrder.class)\n    // Basic field mapping with renaming\n    .withField(\"order_id\", UUID, CustomerOrder::orderId)\n    .withField(\"customer_email\", STRING, CustomerOrder::customerEmail)\n\n    // Computed fields\n    .withField(\"total_items\", INTEGER, order -&gt;\n        order.items().stream().mapToInt(Item::quantity).sum())\n    .withField(\"total_amount\", BIG_DECIMAL.withPrecisionScale(10, 2), order -&gt;\n        order.items().stream()\n            .map(item -&gt; item.price().multiply(BigDecimal.valueOf(item.quantity())))\n            .reduce(BigDecimal.ZERO, BigDecimal::add))\n\n    // Complex collections\n    .withField(\"item_skus\", LIST.ofType(STRING), order -&gt;\n        order.items().stream().map(Item::sku).toList())\n\n    // Enum handling\n    .withField(\"status\", ENUM.ofType(OrderStatus.class), CustomerOrder::status)\n\n    // Date/time fields\n    .withField(\"created_at\", LOCAL_DATE_TIME, CustomerOrder::createdAt)\n    .withField(\"created_date\", LOCAL_DATE, order -&gt; order.createdAt().toLocalDate())\n\n    // Map fields\n    .withField(\"metadata\", MAP.ofTypes(STRING, STRING), CustomerOrder::metadata);\n\n// Use the Write Model\ntry (CarpetWriter&lt;CustomerOrder&gt; writer = new CarpetWriter.Builder&lt;&gt;(outputStream, CustomerOrder.class)\n        .withWriteRecordModel(writeModel)\n        .build()) {\n    writer.write(orders);\n}\n</code></pre> <p>This example demonstrates field renaming, computed fields, collection handling, enum mapping, date/time processing, and complex data type configuration - all the key features of Carpet's Write Model system.</p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":""},{"location":"getting-started/basic-usage/#creating-records","title":"Creating Records","text":"<p>To use Carpet, start by defining your data structure using Java records. You don't need to generate classes or inherit from Carpet classes:</p> <pre><code>record MyRecord(long id, String name, int size, double value, double percentile) { }\n</code></pre> <p>Carpet provides a writer and a reader with a default configuration and convenience methods.</p>"},{"location":"getting-started/basic-usage/#writing-to-parquet","title":"Writing to Parquet","text":"<p>Carpet can use reflection to define the Parquet file schema and writes all the content of your objects into the file:</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (OutputStream outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-from-parquet","title":"Reading from Parquet","text":"<p>To read a Parquet file, you just need to provide a File and Record class that matches the Parquet schema:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-as-map","title":"Reading as Map","text":"<p>If you don't know the schema of the file, or a Map is valid for your use case, you can deserialize to <code>Map&lt;String, Object&gt;</code>:</p> <pre><code>List&lt;Map&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), Map.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, you can explore more advanced features:</p> <ul> <li>Writer API for detailed write operations</li> <li>Reader API for reading capabilities</li> <li>Data Types for supported data types and nested structures</li> <li>Configuration for customizing Parquet settings</li> </ul>"},{"location":"getting-started/carpetreader-api/","title":"CarpetReader API","text":"<p><code>CarpetReader</code> provides multiple ways to read data from Parquet files. When you instantiate a <code>CarpetReader</code> the file is not opened or read. It's processed when you execute one of its read methods.</p> <p>To instantiate it you need to provide a Java <code>File</code> or a Parquet <code>InputFile</code> and the class of the record you want to read. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetReader&lt;MyRecord&gt; reader = new CarpetReader&lt;&gt;(inputFile, MyRecord.class);\n</code></pre> <p>Parquet doesn't support <code>InputStream</code> because Parquet's file format requires random access to read metadata from the footer and data pages throughout the file. Since <code>InputStream</code> only provides sequential forward-only access, it's not suitable for reading Parquet files.</p>"},{"location":"getting-started/carpetreader-api/#reading-methods","title":"Reading Methods","text":""},{"location":"getting-started/carpetreader-api/#stream-processing","title":"Stream Processing","text":"<pre><code>Stream&lt;T&gt; stream()\n</code></pre> <p><code>CarpetReader&lt;T&gt;</code> can return a Java stream to iterate it applying functional logic to filter and transform its content.</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nList&lt;OtherType&gt; list = reader.stream()\n    .filter(r -&gt; r.value() &gt; 100.0)\n    .map(this::mapToOtherType)\n    .toList();\n</code></pre> <p>File content is read while streaming, not loaded entirely into memory. This is useful for large files. The stream will be closed automatically when the processing is done.</p>"},{"location":"getting-started/carpetreader-api/#collecting-tolist","title":"Collecting <code>toList</code>","text":"<p>If you don't need to filter or convert the content, you can directly collect the whole content as a <code>List&lt;T&gt;</code>:</p> <pre><code>List&lt;MyRecord&gt; list = new CarpetReader&lt;&gt;(file, MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/carpetreader-api/#for-each-loop","title":"For-Each Loop","text":"<p><code>CarpetReader&lt;T&gt;</code> implements <code>Iterable&lt;T&gt;</code> and thanks to For-Each Loop feature from Java sintax you can iterate it with a simple for:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nfor (MyRecord r: reader) {\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetreader-api/#iterator","title":"Iterator","text":"<p>Implementing <code>Iterable&lt;T&gt;</code>, there is also available a method <code>iterator()</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nIterator&lt;MyRecord&gt; iterator = reader.iterator();\nwhile (iterator.hasNext()) {\n    MyRecord r = iterator.next();\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetwriter-api/","title":"CarpetWriter API","text":"<p><code>CarpetWriter</code> provides multiple methods for writing data to Parquet files.</p> <p>To instantiate it you need to provide an <code>OutputStream</code> or a Parquet <code>OutputFile</code> and the class of the record you want to write. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class);\n</code></pre>"},{"location":"getting-started/carpetwriter-api/#writing-methods","title":"Writing Methods","text":"<ul> <li><code>void write(T value)</code>: Write a single element. Can be called repeatedly.</li> <li><code>void accept(T value)</code>: Implementing <code>Consumer&lt;T&gt;</code> interface, write a single element. Created to be used in functional processes. If there is an <code>IOException</code>, it is wrapped with a <code>UncheckedIOException</code></li> <li><code>void write(Collection&lt;T&gt; collection)</code>: iterates and serializes a whole collection. Can be any type of <code>Collection</code> implementation.</li> <li><code>void write(Stream&lt;T&gt; stream)</code>: consumes a stream and serializes its values.</li> </ul> <p>You can call repeatedly to all methods in any combination if needed.</p>"},{"location":"getting-started/carpetwriter-api/#usage-example","title":"Usage Example","text":"<pre><code>var outputFile = new FileSystemOutputFile(new File(\"my_file.parquet\"));\ntry (var writer = new CarpetWriter&lt;MyRecord&gt;(outputStream, MyRecord.class)) {\n    // Write single element\n    writer.write(new MyRecord(\"foo\"));\n\n    // Write collection\n    writer.write(List.of(new MyRecord(\"bar\")));\n\n    // Write stream\n    writer.write(Stream.of(new MyRecord(\"foobar\")));\n}\n</code></pre> <p><code>CarpetWriter</code> needs to be closed, and implements <code>Closeable</code> interface to be used in try-with-resources.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#maven","title":"Maven","text":"<p>Include Carpet in your Java project using Maven by adding this dependency to your <code>pom.xml</code>:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.5.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/installation/#gradle","title":"Gradle","text":"<p>If you're using Gradle, add this to your <code>build.gradle</code>:</p> <pre><code>implementation 'com.jerolba:carpet-record:0.5.0'\n</code></pre>"},{"location":"getting-started/installation/#transitive-dependencies-and-hadoop","title":"Transitive dependencies and Hadoop","text":"<p>Carpet is designed to work with local filesystems by default and includes only the minimal Parquet-related dependencies needed for read/write operations.</p> <p>While Parquet was originally developed as part of the Hadoop ecosystem, Carpet explicitly excludes most Hadoop-related transitive dependencies to keep the library lightweight.</p> <p>If you need Hadoop functionality (like HDFS support), you'll need to explicitly add Hadoop dependencies to your project.</p>"}]}