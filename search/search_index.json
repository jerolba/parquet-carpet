{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Carpet: Parquet Serialization and Deserialization Library for Java","text":"<p>A Java library for serializing and deserializing Parquet files efficiently using Java records. This library provides a simple and user-friendly API for working with Parquet files, making it easy to read and write data in the Parquet format in your Java applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Serialize Java records to Parquet files</li> <li>Deserialize Parquet files to Java records</li> <li>Support nested data structures</li> <li>Support nested Collections and Maps</li> <li>Very simple API</li> <li>Low level configuration of Parquet properties</li> <li>Low overhead processing files</li> <li>Minimized <code>parquet-java</code> and hadoop transitive dependencies</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Add the dependency to your project:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>implementation 'com.jerolba:carpet-record:0.4.0'\n</code></pre> <p>Write and read your data:</p> <pre><code>// Define your data structure\nrecord MyRecord(long id, String name, int size, double value) { }\n\n// Write to Parquet\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (var writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n\n// Read from Parquet\nList&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre> <p>Check out the Getting Started guide for more details.</p>"},{"location":"advanced/column-mapping/","title":"Column Name Mapping","text":"<p>Carpet uses reflection to discover the schema of your files. Since Java record attribute names are limited by Java syntax while Parquet column names are more flexible.</p>"},{"location":"advanced/column-mapping/#column-name-aliasing","title":"Column Name Aliasing","text":"<p>You can use the <code>@Alias</code> annotation to specify a different name for a field in the Parquet schema. This is useful when you want to map a Java field to a Parquet column with a different name or format.</p> <pre><code>record MyRecord(long id, String name, @Alias(\"$name.id\") String nameId) { }\n</code></pre>"},{"location":"advanced/column-mapping/#column-name-conversion","title":"Column Name Conversion","text":"<p>Carpet supports automatic conversion of Java field names to Parquet column names. By default, it uses the same name as the field. However, you can modify this behaviour while configuring Carpet.</p>"},{"location":"advanced/column-mapping/#writing","title":"Writing","text":"<p>Writing a file, configure the property <code>columnNamingStrategy</code>:</p> <pre><code>record MyRecord(long userCode, String userName) { }\n\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n    .withColumnNamingStrategy(ColumnNamingStrategy.SNAKE_CASE)\n    .build()) {\n  writer.write(data);\n}\n</code></pre> <p>This will create a Parquet file with columns named <code>user_code</code> and <code>user_name</code>:</p> <pre><code>message MyRecord {\n  required int64 user_code;\n  optional binary user_name (STRING);\n}\n</code></pre> <p>At the moment, only the snake conversion strategy is implemented.</p>"},{"location":"advanced/column-mapping/#reading","title":"Reading","text":"<p>To read a file using the inverse logic we must configure the property <code>fieldMatchingStrategy</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(input, SomeEntity.class)\n    .withFieldMatchingStrategy(FieldMatchingStrategy.SNAKE_CASE);\nList&lt;SomeEntity&gt; list = reader.toList();\n</code></pre> <p>Available strategies reading a file are:</p> <ul> <li><code>FIELD_NAME</code>: Match column with exact field name</li> <li><code>SNAKE_CASE</code>: Match column with snake_case version of field name</li> <li><code>BEST_EFFORT</code>: Try exact match first, then try snake_case</li> </ul> <p>Reading and writing a file, @Alias annotation has precedence over the strategy configuration.</p>"},{"location":"advanced/configuration/","title":"Advanced Configuration","text":"<p>Parquet library provides multiple configuration options to customize the behavior of the writer and reader. <code>CarpetWriter</code> and <code>CarpetReader</code> hide most of these options, but you can still access them if needed.</p> <p>On the other hand, <code>Carpet</code> requires some optional configuration to setup how to handle specific types, such as <code>BigDecimal</code> and <code>LocalDateTime</code>.</p> <p>This section will cover the advanced configurations available in Carpet.</p>"},{"location":"advanced/configuration/#writer-configuration","title":"Writer Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration","title":"Parquet Configuration","text":"<p>Default <code>CarpetWriter</code> constructors cover default <code>ParquetWriter</code> configuration. You can customize Parquet configuration using <code>CarpetWriter.Builder</code>, that exposes all configuration methods if you need to tune it (compression, sizes, hadoop usage, etc).</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n    .withWriteMode(Mode.OVERWRITE)\n    .withCompressionCodec(CompressionCodecName.GZIP)\n    .withPageRowCountLimit(100_000)\n    .withBloomFilterEnabled(\"name\", true)\n    .build()) {\nwriter.write(data);\n</code></pre> <p>Any <code>ParquetWriter</code> configuration can be set using the <code>CarpetWriter.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration","title":"Carpet Configuration","text":"<p>Carpet provides some global configuration options to customize the default behavior of the writer managing some types.</p>"},{"location":"advanced/configuration/#bigdecimal-precision-and-scale","title":"BigDecimal precision and scale","text":"<p>DECIMAL type requires specifying both precision and scale when persisting values. This configuration is set globally when writing a file:</p> <pre><code>record MyRecord(String id, String name, BigDecimal price) { }\n\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(precision, scale)\n        .build()) {\n</code></pre> <p>There is no default value. If <code>BigDecimal</code> type is encountered, but precision and scale are not configured, Carpet throws an exception.</p> <p>If a <code>BigDecimal</code> value has a higher scale than the configured scale, Carpet does not rescale it by default and instead it throws an exception. To prevent this and automatically rescale values to the configured scale, you must specify the <code>RoundingMode</code> using the <code>withBigDecimalScaleAdjustment</code> method:</p> <pre><code>try (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(20, 3)\n        .withBigDecimalScaleAdjustment(RoundingMode.HALF_UP)\n        .build()) {\n    writer.write(new MyRecord(\"1\", \"item1\", new BigDecimal(\"123.45678\")));\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>BigDecimal</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#time-unit-configuration","title":"Time-Unit Configuration","text":"<p>TIME and TIMESTAMP Parquet types support configuring the decimal second unit (<code>MILLIS</code>, <code>MICROS</code> or <code>NANOS</code>).</p> <p>In Carpet, Time-Unit configuration is global when writing a file, and by default it's configured as <code>MILLIS</code>.</p> <p>The global configuration can be overwritten in the CarpetWriter builder:</p> <pre><code>record MyRecord(long itemId, int count, LocalTime saleTime) { }\n\nvar writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n        .withDefaultTimeUnit(TimeUnit.MICROS);\n        .build()) {\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>LocalTime</code>, <code>LocalDateTime</code> and <code>Instant</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#reader-configuration","title":"Reader Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration_1","title":"Parquet Configuration","text":"<p>CarpetReader doesn't provide a builder. It has been simplified to just provide Carpet specific configuration. You can still access all <code>ParquetReader</code> configuration options using the <code>CarpetParquetReader.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration_1","title":"Carpet Configuration","text":"<p>CarpetReader provides some configuration options to customize the behavior of the reader matching the schema of the file with the schema of the class used to read it.</p> <p>Configure how schema mismatches are handled:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    // Fail on null values for primitives\n    .withFailOnNullForPrimitives(true)\n    // Allow missing columns in the file\n    .withFailOnMissingColumn(false)\n    // Prevent narrowing conversions\n    .withFailNarrowingPrimitiveConversion(true)\n    // Flexible name matching\n    .withFieldMatchingStrategy(FieldMatchingStrategy.BEST_EFFORT);\n</code></pre>"},{"location":"advanced/data-types/","title":"Supported Data Types","text":""},{"location":"advanced/data-types/#basic-types","title":"Basic Types","text":"<p>Carpet maps main Java types to Parquet data types automatically:</p> Java Type Parquet Type boolean/Boolean boolean byte/Byte int32 short/Short int32 int/Integer int32 long/Long int64 float/Float float double/Double double Binary binary String binary (STRING) Enum binary (ENUM) UUID fixed_len_byte_array(16) (UUID)"},{"location":"advanced/data-types/#temporal-types","title":"Temporal Types","text":"<p>Support for Java time types:</p> Java Type Parquet Type LocalDate int32 (DATE) LocalTime int32 (TIME(MILLIS|MICROS)) or int64 (TIME(NANOS)) LocalDateTime int64 (TIMESTAMP(MILLIS|MICROS|NANOS)) Instant int64 (TIMESTAMP(MILLIS|MICROS|NANOS))"},{"location":"advanced/data-types/#decimal-numbers","title":"Decimal Numbers","text":"<p>BigDecimal mapping depends on precision:</p> Precision Parquet Type \u2264 9 int32 (DECIMAL) \u2264 18 int64 (DECIMAL) &gt; 18 binary (DECIMAL) or fixed_len_byte_array (DECIMAL)"},{"location":"advanced/data-types/#binary","title":"Binary","text":"<p>Carpet supports storing binary data using the <code>org.apache.parquet.io.api.Binary</code> class. This is useful for storing raw binary data that doesn't fit into other types. The <code>Binary</code> class provides methods for creating and manipulating binary data.</p> <p>Following record:</p> <pre><code>record SimpleRecord(long id, Binary data) { }\n</code></pre> <p>generates a Parquet schema with a <code>binary</code> type:</p> <pre><code>message SimpleRecord {\n    required int64 id;\n    optional binary data;\n}\n</code></pre>"},{"location":"advanced/data-types/#json-and-bson-types","title":"JSON and BSON types","text":"<p>Java doesn't have a native JSON or BSON type, but you can use <code>String</code> or <code>org.apache.parquet.io.api.Binary</code> to store JSON or BSON data.</p> <p>To configure it, you can use the <code>@ParquetJson</code> or <code>@ParquetBson</code> annotations to specify the logical type in Parquet schema.</p> <p>You can find more information about JSON and BSON in the Java Type Annotations section.</p>"},{"location":"advanced/data-types/#nested-structures","title":"Nested Structures","text":"<p>Parquet supports nested structures, and Carpet can generate them using Java records and Collections.</p> <p>Carpet uses the following rules to generate nested structures:</p> <ol> <li>Types must be concrete and cannot be generic.</li> <li>Types cannot be recursive directly or indirectly.</li> </ol>"},{"location":"advanced/data-types/#nested-records","title":"Nested Records","text":"<p>Carpet supports nested records to create files with structured data. There is one exception: types can not be recursive directly nor indirectly.</p> <pre><code>record Address(String street, String zip, String city) { }\nrecord Job(String company, String position, int years) { }\nrecord Person(long id, Job job, Address address) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Person.class)) {\n    var president = new Person(1010101, new Job(\"USA\", POTUS, 3),\n        new Address(\"1600 Pennsylvania Av.\", \"20500\", \"Washington\"));\n    writer.write(president));\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Person {\n  required int64 id;\n  optional group job {\n    optional binary company (STRING);\n    optional binary position (STRING);\n    required int32 years;\n  }\n  optional group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre>"},{"location":"advanced/data-types/#collections","title":"Collections","text":"<p>Carpet supports nested collections to create files with structured data. Collection elements must be one of the supported types.</p> <pre><code>record Line(String sku, int quantity, double price){ }\nrecord Invoice(String id, double amount, double taxes, List&lt;Line&gt; lines) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Invoice.class)) {\n    var invoice = new Invoice(\"2023/211\", 2323.23, 232.32, List.of(\n        new Line(\"AAA\", 3, 500.0), new Line(\"BBB\", 1, 823.23)));\n    writer.write(invoice);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a collection to any type of Java <code>Collection</code> implementation. The only restriction is that the Collection type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#maps","title":"Maps","text":"<p>Carpet supports nested maps to create files with structured data. Map elements must be one of the supported types.</p> <pre><code>record State(double area, int population){ }\nrecord Country(String name, double area, Map&lt;String, State&gt; states) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Country.class)) {\n    var country = new Country(\"USA\", 9_833_520.0, Map.of(\n        \"Idaho\", new State(216_444.0, 1_975_000),\n        \"Texas\", new State(695_662.0, 29_145_505)));\n    writer.write(country);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Country {\n  optional binary name (STRING);\n  required double area;\n  optional group states (MAP) {\n    repeated group key_value {\n      required binary key (STRING);\n      optional group value {\n        required double area;\n        required int32 population;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a map to any type of Java <code>Map</code> implementation. The only restriction is that the Map type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#limitations","title":"Limitations","text":""},{"location":"advanced/data-types/#generic-types","title":"Generic Types","text":"<p>Records cannot have generic elements as Carpet needs concrete type information to create the Parquet schema. This includes generic records, collections, and maps.</p> <pre><code>// This will NOT work\nrecord WithGeneric&lt;T&gt;(String name, T child) { }\n\n// This works fine\nrecord WithList(String name, List&lt;String&gt; items) { }\nrecord WithMap(String name, Map&lt;String, Integer&gt; values) { }\n</code></pre> <p>If generic type is used, <code>RecordTypeConversionException</code> will be thrown.</p> <p>Collections and Maps with concrete types don't have this issue because Carpet knows their concrete type information at compile time.</p>"},{"location":"advanced/data-types/#recursive-types","title":"Recursive Types","text":"<p>Records cannot have direct or indirect recursive references:</p> <pre><code>// This will NOT work - direct recursion\nrecord Node(String id, Node next) { }\n\n// This will NOT work - indirect recursion\nrecord Child(String id, Parent parent) { }\nrecord Parent(String id, Child child) { }\n</code></pre>"},{"location":"advanced/input-output-files/","title":"Input and Output Files","text":"<p><code>parquet-java</code> defines <code>OutputFile</code> and <code>InputFile</code> interfaces to interact with files. Originally, it only provided <code>HadoopOutputFile</code> and <code>HadoopInputFile</code> implementations that were capable of working with Hadoop and local files.</p> <p>This required a Hadoop dependency to be included in the project. This is not ideal for projects that only need to work with local files, as it adds unnecessary complexity and size to the project. To address this, Parquet Java recently added <code>LocalOutputFile</code> and <code>LocalInputFile</code> implementations.</p> <p>Before these classes were created, Carpet provided a local file implementation with <code>FileSystemOutputFile</code> and <code>FileSystemInputFile</code>. You can use either implementation.</p>"},{"location":"advanced/java-type-annotations/","title":"Java Type Annotations","text":"<p>Carpet allows you to store <code>String</code>, Enum, or <code>org.apache.parquet.io.api.Binary</code> fields as the Parquet <code>BINARY</code> type with different logical types, such as String, Enum, JSON, or BSON. This is useful for embedding JSON, BSON documents, or any raw binary data directly into your Parquet files if no native type is available for your use case.</p> <p>You can use the <code>@ParquetString</code>, <code>@ParquetEnum</code>, <code>@ParquetJson</code>, or <code>@ParquetBson</code> annotations to configure the logical type in the Parquet schema. These annotations do not transform or convert the actual data. They simply specify how the data should be interpreted in the Parquet format. Carpet does not validate the content of the data, so you must ensure that the data you are writing is valid String, JSON, or BSON.</p> <p>These annotations can be applied to record components or collection elements (<code>List&lt;@ParquetBson Binary&gt; values</code>). The following sections describe how to use these annotations with different types.</p>"},{"location":"advanced/java-type-annotations/#parquetstring-annotation","title":"@ParquetString annotation","text":"<p>The <code>@ParquetString</code> annotation is used to specify that a field should be stored as a Parquet string type when the field type is an Enum or a Parquet Java <code>Binary</code> type.</p> <p>By default, Carpet converts <code>Binary</code> and Enum types to their corresponding Parquet types. However, for some use cases, you may want to store them as binary strings instead, overriding the default behavior.</p>"},{"location":"advanced/java-type-annotations/#with-binary-type","title":"With Binary type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetString Binary code) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary code (STRING);\n}\n</code></pre> <p>This is useful when the source of your information is a <code>Binary</code> type, but you still want to store it as a string in Parquet.</p>"},{"location":"advanced/java-type-annotations/#with-enum-type","title":"With Enum type","text":"<p>The following record:</p> <pre><code>enum Category { HIGH, MEDIUM, LOW }\n\nrecord Person(String name, @ParquetString Category category) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary category (STRING);\n}\n</code></pre> <p>You can work with enumerations while keeping their String representation in Parquet, without breaking contracts with other systems.</p>"},{"location":"advanced/java-type-annotations/#parquetenum-annotation","title":"@ParquetEnum annotation","text":"<p>The <code>@ParquetEnum</code> annotation is used to specify that a field should be stored as a Parquet enum type when the field type is a <code>String</code> or a Parquet Java <code>Binary</code> type.</p> <p>By default, Carpet converts <code>Binary</code> and <code>String</code> types to their corresponding Parquet types. However, for some use cases, you may want to store them as binary Enum instead, overriding the default behavior.</p>"},{"location":"advanced/java-type-annotations/#with-binary-type_1","title":"With Binary type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetEnum Binary code) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary code (ENUM);\n}\n</code></pre> <p>This is useful when the source of your information is a <code>Binary</code> type, but you still want to store it as an Enum in Parquet.</p>"},{"location":"advanced/java-type-annotations/#with-string-type","title":"With String type","text":"<p>The following record:</p> <pre><code>record Person(String name, @ParquetEnum String category) { }\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Person {\n  optional binary name (STRING);\n  optional binary category (ENUM);\n}\n</code></pre> <p>You can work with Strings while keeping their Enum representation in Parquet, without breaking contracts with other systems.</p>"},{"location":"advanced/java-type-annotations/#parquetjson-annotation","title":"@ParquetJson annotation","text":"<p>Java does not have a native JSON type, but you can use <code>String</code> or <code>Binary</code> to store JSON data. The <code>@ParquetJson</code> annotation is used to specify that a field should be stored as a Parquet JSON type when the field type is a <code>String</code> or <code>Binary</code>.</p> <p>To store a field as JSON, annotate the record component with <code>@ParquetJson</code>. The data will be stored as Parquet <code>binary</code> with the <code>JSON</code> logical type.</p> <p>The following record:</p> <pre><code>record ProductEvent(long id, Instant timestamp, @ParquetJson String jsonData){}\n</code></pre> <p>generates a schema with a <code>binary</code> field annotated with the <code>JSON</code> logical type:</p> <pre><code>message ProductEvent {\n    required int64 id;\n    required int64 timestamp (TIMESTAMP(MILLIS,true));\n    optional binary jsonData (JSON);\n}\n</code></pre> <p><code>@ParquetJson</code> can also annotate the <code>Binary</code> class.</p>"},{"location":"advanced/java-type-annotations/#parquetbson-annotation","title":"@ParquetBson annotation","text":"<p>Similar to JSON, Java does not have a native BSON type, but you can use the <code>Binary</code> type to store BSON data. The <code>@ParquetBson</code> annotation is used to specify that a field should be stored as a Parquet BSON type when the field type is <code>Binary</code>.</p> <p>The following record:</p> <pre><code>record ProductEvent(long id, Instant timestamp, @ParquetBson Binary bsonData){}\n</code></pre> <p>generates a schema with a <code>binary</code> field annotated with the <code>BSON</code> logical type:</p> <pre><code>message ProductEvent {\n    required int64 id;\n    required int64 timestamp (TIMESTAMP(MILLIS,true));\n    optional binary bsonData (BSON);\n}\n</code></pre> <p>Carpet does not validate the content of the data, so you must ensure that the data you are writing is valid BSON.</p>"},{"location":"advanced/java-type-annotations/#bigdecimal-type","title":"BigDecimal type","text":"<p>The <code>BigDecimal</code> type is used to represent arbitrary-precision decimal numbers. In Parquet, <code>BigDecimal</code> can be represented by multiple physical Parquet types, all configured with the <code>DECIMAL</code> logical type and a specified precision and scale.</p>"},{"location":"advanced/java-type-annotations/#precisionscale-annotation","title":"@PrecisionScale annotation","text":"<p>The precision is the total number of digits, and the scale is the number of digits to the right of the decimal point.</p> <p>When writing a file, the precision and scale can be configured globally in the writer configuration or per record field using the <code>@PrecisionScale</code> annotation. Annotation configuration takes precedence over the writer configuration.</p> <p>The following record:</p> <pre><code>record Product(long id, @PrecisionScale(20, 4) BigDecimal price) {}\n</code></pre> <p>will be converted to the following Parquet schema:</p> <pre><code>message Product {\n  required int64 id;\n  optional binary price (DECIMAL(20,4));\n}\n</code></pre> <p>When writing a file with a configured precision and scale, Carpet adapts the data to these specifications. If the data in the file has a different precision or scale, it will be converted to the specified precision and scale.</p> <p>When reading a file with a record field annotated with <code>@PrecisionScale</code>, Carpet does NOT validate the precision and scale of the data. It reads the data as <code>BigDecimal</code> using the precision and scale from the file. If the data in the file has a different precision or scale, Carpet will not throw an error or convert it. You must ensure that the data you are reading is valid for the specified precision and scale.</p>"},{"location":"advanced/java-type-annotations/#rounding-annotation","title":"@Rounding annotation","text":"<p>If scale adjustment is needed, you must configure the rounding mode to round the value to the specified scale.</p> <p>When writing a file, the rounding mode can be configured globally in the writer configuration or per record field using the <code>@Rounding</code> annotation. Annotation configuration takes precedence over the writer configuration.</p> <p>The <code>@Rounding</code> annotation requires a <code>RoundingMode</code> enum parameter, which is used to round <code>BigDecimal</code> values in the Java API. This annotation does not modify the generated Parquet schema but configures the rounding mode for <code>BigDecimal</code> values.</p> <pre><code>record Product(\n    long id,\n    @PrecisionScale(20, 4) @Rounding(RoundingMode.HALF_UP) BigDecimal price) {\n}\n</code></pre> <p>If the rounding mode is not specified via annotation or writer configuration, the default is <code>RoundingMode.UNNECESSARY</code>. This means an exception will be thrown if rounding is necessary, which is useful to ensure data integrity if no changes are expected during conversion.</p> <p><code>@PrecisionScale</code> and <code>@Rounding</code> annotations can be used together or separately, depending on your use case and how you want to configure the precision and scale of <code>BigDecimal</code> values in your Parquet files.</p>"},{"location":"advanced/low-level-parquet/","title":"Low level Parquet classes","text":"<p>Carpet is built on top of parquet-java library and supports creating native library <code>ParquetWriter</code> and <code>ParquetReader</code> classes, and use it with third party libraries that work with Parquet classes.</p>"},{"location":"advanced/low-level-parquet/#parquetwriter","title":"ParquetWriter","text":"<pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\nPath path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nOutputFile outputFile = HadoopOutputFile.fromPath(path, new Configuration());\ntry (ParquetWriter&lt;MyRecord&gt; writer = CarpetParquetWriter.builder(outputFile, MyRecord.class)\n        .withWriteMode(Mode.OVERWRITE)\n        .withCompressionCodec(CompressionCodecName.GZIP)\n        .withPageRowCountLimit(100_000)\n        .withBloomFilterEnabled(\"name\", true)\n        .build()) {\n\n    otherLibraryIntegrationWrite(writer, data);\n}\n</code></pre>"},{"location":"advanced/low-level-parquet/#parquetreader","title":"ParquetReader","text":"<pre><code>Path path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nInputFile inputFile = new HadoopInputFile(path, new Configuration());\ntry (ParquetReader&lt;MyRecord&gt; reader = CarpetParquetReader.builder(inputFile, MyRecord.class).build()) {\n    var data = otherLibraryIntegrationRead(reader);\n    // process data\n}\n</code></pre>"},{"location":"advanced/nullability/","title":"Nullability","text":"<p>Parquet supports to configure not null columns in the schema. Carpet, writing the schema, respects Java primitives' nullability:</p> <p>This record:</p> <pre><code>record MyRecord(long id, String name, int size, double value){ }\n</code></pre> <p>generates this schema with primitive types as <code>required</code>:</p> <pre><code>message MyRecord {\n  required int64 id;\n  optional binary name (STRING);\n  required int32 size;\n  required double value;\n}\n</code></pre> <p>while this record:</p> <pre><code>record MyRecord(Long id, String name, Integer size, Double value) { }\n</code></pre> <p>generates this schema with all numeric values as <code>optional</code>:</p> <pre><code>message MyRecord {\n  optional int64 id;\n  optional binary name (STRING);\n  optional int32 size;\n  optional double value;\n}\n</code></pre> <p>String, List or Map types are objects and can be nullable. To generate a schema where an object reference field is created as <code>required</code> you must annotate the field with <code>@NotNull</code> annotation.</p> <pre><code>record MyRecord(@NotNull String id, @NotNull String name, @NotNull Address address){ }\n</code></pre> <p>generates this schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  required group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre> <p>The <code>@NotNull</code> annotation is not part of the Java standard library and Carpet provides one implementation. You can use any library that provides this type of annotation, such as <code>javax.validation.constraints.NotNull</code> or <code>jakarta.annotation.Nonnull</code>. Carpet inspects fields annotation looking by the name of the annotation not the complete type.</p>"},{"location":"advanced/projections/","title":"Projections","text":"<p>One of key features of Parquet is that you can save a lot of I/O and CPU if you read only a subset of available columns in a file.</p> <p>Given a parquet file, you can read a subset of columns just using a Record with needed columns.</p> <p>For example, from a file with this schema, you can read just id, sku, and quantity fields:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  required double amount;\n  required double taxes;\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>defining this records:</p> <pre><code>record LineRead(String sku, int quantity) { }\n\nrecord InvoiceRead(String id, List&lt;LineRead&gt; lines) { }\n\nList&lt;InvoiceRead&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), InvoiceRead.class).toList();\n</code></pre> <p>Parquet will read and parse only pages with id, sku, and quantity columns, skipping the rest of the file.</p>"},{"location":"advanced/schema-mismatch/","title":"Read Schema mismatch","text":"<p>How does Carpet behave when the schema does not exactly match records types?</p>"},{"location":"advanced/schema-mismatch/#nullable-column-mapped-to-primitive-type","title":"Nullable column mapped to primitive type","text":"<p>By default Carpet doesn't fail when a column is defined as <code>optional</code> but the record field is primitive.</p> <p>This parquet schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  optional int32 age;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>When a null value appears in a file, the field is filled with the default value of the primitive (0, 0.0 or false).</p> <p>If you want to ensure that the application fails if an optional column is mapped to a primitive field, you must enable the flag <code>FailOnNullForPrimitives</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnNullForPrimitives(true)\n    .toList();\n</code></pre> <p>By default, <code>FailOnNullForPrimitives</code> value is false.</p>"},{"location":"advanced/schema-mismatch/#missing-fields","title":"Missing fields","text":"<p>When parquet file schema doesn't match with existing record fields, Carpet throws an exception.</p> <p>This schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n}\n</code></pre> <p>is not compatible with this record because it contains an additional <code>int age</code> field:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>If for some reason you are forced to read the file with an incompatible record, you can disable the schema compatibility check with flag <code>FailOnMissingColumn</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnMissingColumn(false)\n    .toList();\n</code></pre> <p>Carpet will skip the schema verification and fill the value with <code>null</code> in case of Objects or the default value of primitives (0, 0.0 or false).</p> <p>By default, <code>FailOnMissingColumn</code> value is true.</p> <p>If a column that exists in the file is not present in the record, Carpet will ignore it and will not throw an exception because it's considered a projection.</p>"},{"location":"advanced/schema-mismatch/#narrowing-numeric-values","title":"Narrowing numeric values","text":"<p>By default Carpet converts between numeric types:</p> <ul> <li>Any integer type can be converted to another integer type of different size: byte &lt;-&gt; short &lt;-&gt; int &lt;-&gt; long.</li> <li>Any decimal type can be converted to another decimal type of different size: float &lt;-&gt; double</li> </ul> <p>This schema</p> <pre><code>message MyRecord {\n  required int64 id;\n  required double value;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(int id, float value) { }\n</code></pre> <p>Carpet will cast numeric types using Narrowing Primitive Conversion rules from Java.</p> <p>If you want to ensure that the application fails if a type is converted to a narrow value, you can enable the flag <code>FailNarrowingPrimitiveConversion</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailNarrowingPrimitiveConversion(true)\n    .toList();\n</code></pre> <p>By default, <code>FailNarrowingPrimitiveConversion</code> value is false.</p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":""},{"location":"getting-started/basic-usage/#creating-records","title":"Creating Records","text":"<p>To use Carpet, start by defining your data structure using Java records. You don't need to generate classes or inherit from Carpet classes:</p> <pre><code>record MyRecord(long id, String name, int size, double value, double percentile) { }\n</code></pre> <p>Carpet provides a writer and a reader with a default configuration and convenience methods.</p>"},{"location":"getting-started/basic-usage/#writing-to-parquet","title":"Writing to Parquet","text":"<p>Carpet can use reflection to define the Parquet file schema and writes all the content of your objects into the file:</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (OutputStream outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-from-parquet","title":"Reading from Parquet","text":"<p>To read a Parquet file, you just need to provide a File and Record class that matches the Parquet schema:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-as-map","title":"Reading as Map","text":"<p>If you don't know the schema of the file, or a Map is valid for your use case, you can deserialize to <code>Map&lt;String, Object&gt;</code>:</p> <pre><code>List&lt;Map&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), Map.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, you can explore more advanced features:</p> <ul> <li>Writer API for detailed write operations</li> <li>Reader API for reading capabilities</li> <li>Data Types for supported data types and nested structures</li> <li>Configuration for customizing Parquet settings</li> </ul>"},{"location":"getting-started/carpetreader-api/","title":"CarpetReader API","text":"<p><code>CarpetReader</code> provides multiple ways to read data from Parquet files. When you instantiate a <code>CarpetReader</code> the file is not opened or read. It's processed when you execute one of its read methods.</p> <p>To instantiate it you need to provide a Java <code>File</code> or a Parquet <code>InputFile</code> and the class of the record you want to read. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetReader&lt;MyRecord&gt; reader = new CarpetReader&lt;&gt;(inputFile, MyRecord.class);\n</code></pre> <p>Parquet doesn't support <code>InputStream</code> because Parquet's file format requires random access to read metadata from the footer and data pages throughout the file. Since <code>InputStream</code> only provides sequential forward-only access, it's not suitable for reading Parquet files.</p>"},{"location":"getting-started/carpetreader-api/#reading-methods","title":"Reading Methods","text":""},{"location":"getting-started/carpetreader-api/#stream-processing","title":"Stream Processing","text":"<pre><code>Stream&lt;T&gt; stream()\n</code></pre> <p><code>CarpetReader&lt;T&gt;</code> can return a Java stream to iterate it applying functional logic to filter and transform its content.</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nList&lt;OtherType&gt; list = reader.stream()\n    .filter(r -&gt; r.value() &gt; 100.0)\n    .map(this::mapToOtherType)\n    .toList();\n</code></pre> <p>File content is read while streaming, not loaded entirely into memory. This is useful for large files. The stream will be closed automatically when the processing is done.</p>"},{"location":"getting-started/carpetreader-api/#collecting-tolist","title":"Collecting <code>toList</code>","text":"<p>If you don't need to filter or convert the content, you can directly collect the whole content as a <code>List&lt;T&gt;</code>:</p> <pre><code>List&lt;MyRecord&gt; list = new CarpetReader&lt;&gt;(file, MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/carpetreader-api/#for-each-loop","title":"For-Each Loop","text":"<p><code>CarpetReader&lt;T&gt;</code> implements <code>Iterable&lt;T&gt;</code> and thanks to For-Each Loop feature from Java sintax you can iterate it with a simple for:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nfor (MyRecord r: reader) {\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetreader-api/#iterator","title":"Iterator","text":"<p>Implementing <code>Iterable&lt;T&gt;</code>, there is also available a method <code>iterator()</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nIterator&lt;MyRecord&gt; iterator = reader.iterator();\nwhile (iterator.hasNext()) {\n    MyRecord r = iterator.next();\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetwriter-api/","title":"CarpetWriter API","text":"<p><code>CarpetWriter</code> provides multiple methods for writing data to Parquet files.</p> <p>To instantiate it you need to provide an <code>OutputStream</code> or a Parquet <code>OutputFile</code> and the class of the record you want to write. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class);\n</code></pre>"},{"location":"getting-started/carpetwriter-api/#writing-methods","title":"Writing Methods","text":"<ul> <li><code>void write(T value)</code>: Write a single element. Can be called repeatedly.</li> <li><code>void accept(T value)</code>: Implementing <code>Consumer&lt;T&gt;</code> interface, write a single element. Created to be used in functional processes. If there is an <code>IOException</code>, it is wrapped with a <code>UncheckedIOException</code></li> <li><code>void write(Collection&lt;T&gt; collection)</code>: iterates and serializes a whole collection. Can be any type of <code>Collection</code> implementation.</li> <li><code>void write(Stream&lt;T&gt; stream)</code>: consumes a stream and serializes its values.</li> </ul> <p>You can call repeatedly to all methods in any combination if needed.</p>"},{"location":"getting-started/carpetwriter-api/#usage-example","title":"Usage Example","text":"<pre><code>var outputFile = new FileSystemOutputFile(new File(\"my_file.parquet\"));\ntry (var writer = new CarpetWriter&lt;MyRecord&gt;(outputStream, MyRecord.class)) {\n    // Write single element\n    writer.write(new MyRecord(\"foo\"));\n\n    // Write collection\n    writer.write(List.of(new MyRecord(\"bar\")));\n\n    // Write stream\n    writer.write(Stream.of(new MyRecord(\"foobar\")));\n}\n</code></pre> <p><code>CarpetWriter</code> needs to be closed, and implements <code>Closeable</code> interface to be used in try-with-resources.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#maven","title":"Maven","text":"<p>Include Carpet in your Java project using Maven by adding this dependency to your <code>pom.xml</code>:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/installation/#gradle","title":"Gradle","text":"<p>If you're using Gradle, add this to your <code>build.gradle</code>:</p> <pre><code>implementation 'com.jerolba:carpet-record:0.4.0'\n</code></pre>"},{"location":"getting-started/installation/#transitive-dependencies-and-hadoop","title":"Transitive dependencies and Hadoop","text":"<p>Carpet is designed to work with local filesystems by default and includes only the minimal Parquet-related dependencies needed for read/write operations.</p> <p>While Parquet was originally developed as part of the Hadoop ecosystem, Carpet explicitly excludes most Hadoop-related transitive dependencies to keep the library lightweight.</p> <p>If you need Hadoop functionality (like HDFS support), you'll need to explicitly add Hadoop dependencies to your project.</p>"}]}