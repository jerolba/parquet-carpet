{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Carpet: Parquet Serialization and Deserialization Library for Java","text":"<p>A Java library for serializing and deserializing Parquet files efficiently using Java records. This library provides a simple and user-friendly API for working with Parquet files, making it easy to read and write data in the Parquet format in your Java applications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Serialize Java records to Parquet files</li> <li>Deserialize Parquet files to Java records</li> <li>Support nested data structures</li> <li>Support nested Collections and Maps</li> <li>Very simple API</li> <li>Low level configuration of Parquet properties</li> <li>Low overhead processing files</li> <li>Minimized <code>parquet-java</code> and hadoop transitive dependencies</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Add the dependency to your project:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>implementation 'com.jerolba:carpet-record:0.3.0'\n</code></pre> <p>Write and read your data:</p> <pre><code>// Define your data structure\nrecord MyRecord(long id, String name, int size, double value) { }\n\n// Write to Parquet\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (var writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n\n// Read from Parquet\nList&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre> <p>Check out the Getting Started guide for more details.</p>"},{"location":"advanced/column-mapping/","title":"Column Name Mapping","text":"<p>Carpet uses reflection to discover the schema of your files. Since Java record attribute names are limited by Java syntax while Parquet column names are more flexible.</p>"},{"location":"advanced/column-mapping/#column-name-aliasing","title":"Column Name Aliasing","text":"<p>You can use the <code>@Alias</code> annotation to specify a different name for a field in the Parquet schema. This is useful when you want to map a Java field to a Parquet column with a different name or format.</p> <pre><code>record MyRecord(long id, String name, @Alias(\"$name.id\") String nameId) { }\n</code></pre>"},{"location":"advanced/column-mapping/#column-name-conversion","title":"Column Name Conversion","text":"<p>Carpet supports automatic conversion of Java field names to Parquet column names. By default, it uses the same name as the field. However, you can modify this behaviour while configuring Carpet.</p>"},{"location":"advanced/column-mapping/#writing","title":"Writing","text":"<p>Writing a file, configure the property <code>columnNamingStrategy</code>:</p> <pre><code>record MyRecord(long userCode, String userName) { }\n\nList&lt;MyRecord&gt; data = calculateDataToPersist();\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n    .withColumnNamingStrategy(ColumnNamingStrategy.SNAKE_CASE)\n    .build()) {\n  writer.write(data);\n}\n</code></pre> <p>This will create a Parquet file with columns named <code>user_code</code> and <code>user_name</code>:</p> <pre><code>message MyRecord {\n  required int64 user_code;\n  optional binary user_name (STRING);\n}\n</code></pre> <p>At the moment, only the snake conversion strategy is implemented.</p>"},{"location":"advanced/column-mapping/#reading","title":"Reading","text":"<p>To read a file using the inverse logic we must configure the property <code>fieldMatchingStrategy</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(input, SomeEntity.class)\n    .withFieldMatchingStrategy(FieldMatchingStrategy.SNAKE_CASE);\nList&lt;SomeEntity&gt; list = reader.toList();\n</code></pre> <p>Available strategies reading a file are:</p> <ul> <li><code>FIELD_NAME</code>: Match column with exact field name</li> <li><code>SNAKE_CASE</code>: Match column with snake_case version of field name</li> <li><code>BEST_EFFORT</code>: Try exact match first, then try snake_case</li> </ul> <p>Reading and writing a file, @Alias annotation has precedence over the strategy configuration.</p>"},{"location":"advanced/configuration/","title":"Advanced Configuration","text":"<p>Parquet library provides multiple configuration options to customize the behavior of the writer and reader. <code>CarpetWriter</code> and <code>CarpetReader</code> hide most of these options, but you can still access them if needed.</p> <p>On the other hand, <code>Carpet</code> requires some optional configuration to setup how to handle specific types, such as <code>BigDecimal</code> and <code>LocalDateTime</code>.</p> <p>This section will cover the advanced configurations available in Carpet.</p>"},{"location":"advanced/configuration/#writer-configuration","title":"Writer Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration","title":"Parquet Configuration","text":"<p>Default <code>CarpetWriter</code> constructors cover default <code>ParquetWriter</code> configuration. You can customize Parquet configuration using <code>CarpetWriter.Builder</code>, that exposes all configuration methods if you need to tune it (compression, sizes, hadoop usage, etc).</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n    .withWriteMode(Mode.OVERWRITE)\n    .withCompressionCodec(CompressionCodecName.GZIP)\n    .withPageRowCountLimit(100_000)\n    .withBloomFilterEnabled(\"name\", true)\n    .build()) {\nwriter.write(data);\n</code></pre> <p>Any <code>ParquetWriter</code> configuration can be set using the <code>CarpetWriter.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration","title":"Carpet Configuration","text":"<p>Carpet provides some global configuration options to customize the default behavior of the writer managing some types.</p>"},{"location":"advanced/configuration/#bigdecimal-precision-and-scale","title":"BigDecimal precision and scale","text":"<p>DECIMAL type requires specifying both precision and scale when persisting values. This configuration is set globally when writing a file:</p> <pre><code>record MyRecord(String id, String name, BigDecimal price) { }\n\ntry (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(precision, scale)\n        .build()) {\n</code></pre> <p>There is no default value. If <code>BigDecimal</code> type is encountered, but precision and scale are not configured, Carpet throws an exception.</p> <p>If a <code>BigDecimal</code> value has a higher scale than the configured scale, Carpet does not rescale it by default and instead it throws an exception. To prevent this and automatically rescale values to the configured scale, you must specify the <code>RoundingMode</code> using the <code>withBigDecimalScaleAdjustment</code> method:</p> <pre><code>try (var writer = new CarpetWriter.Builder&lt;&gt;(outputFile, MyRecord.class)\n        .withDefaultDecimal(20, 3)\n        .withBigDecimalScaleAdjustment(RoundingMode.HALF_UP)\n        .build()) {\n    writer.write(new MyRecord(\"1\", \"item1\", new BigDecimal(\"123.45678\")));\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>BigDecimal</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#time-unit-configuration","title":"Time-Unit Configuration","text":"<p>TIME and TIMESTAMP Parquet types support configuring the decimal second unit (<code>MILLIS</code>, <code>MICROS</code> or <code>NANOS</code>).</p> <p>In Carpet, Time-Unit configuration is global when writing a file, and by default it's configured as <code>MILLIS</code>.</p> <p>The global configuration can be overwritten in the CarpetWriter builder:</p> <pre><code>record MyRecord(long itemId, int count, LocalTime saleTime) { }\n\nvar writer = new CarpetWriter.Builder&lt;&gt;(outputStream, MyRecord.class)\n        .withDefaultTimeUnit(TimeUnit.MICROS);\n        .build()) {\n</code></pre> <p>This configuration is only applied when writing the file. When reading, the <code>LocalTime</code>, <code>LocalDateTime</code> and <code>Instant</code> values are read as they are stored in the file, without any adjustment.</p>"},{"location":"advanced/configuration/#reader-configuration","title":"Reader Configuration","text":""},{"location":"advanced/configuration/#parquet-configuration_1","title":"Parquet Configuration","text":"<p>CarpetReader doesn't provide a builder. It has been simplified to just provide Carpet specific configuration. You can still access all <code>ParquetReader</code> configuration options using the <code>CarpetParquetReader.Builder</code>.</p>"},{"location":"advanced/configuration/#carpet-configuration_1","title":"Carpet Configuration","text":"<p>CarpetReader provides some configuration options to customize the behavior of the reader matching the schema of the file with the schema of the class used to read it.</p> <p>Configure how schema mismatches are handled:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    // Fail on null values for primitives\n    .withFailOnNullForPrimitives(true)\n    // Allow missing columns in the file\n    .withFailOnMissingColumn(false)\n    // Prevent narrowing conversions\n    .withFailNarrowingPrimitiveConversion(true)\n    // Flexible name matching\n    .withFieldMatchingStrategy(FieldMatchingStrategy.BEST_EFFORT);\n</code></pre>"},{"location":"advanced/data-types/","title":"Supported Data Types","text":""},{"location":"advanced/data-types/#basic-types","title":"Basic Types","text":"<p>Carpet maps main Java types to Parquet data types automatically:</p> Java Type Parquet Type boolean/Boolean boolean byte/Byte int32 short/Short int32 int/Integer int32 long/Long int64 float/Float float double/Double double String binary (STRING) Enum binary (ENUM) UUID fixed_len_byte_array(16) (UUID)"},{"location":"advanced/data-types/#temporal-types","title":"Temporal Types","text":"<p>Support for Java time types:</p> Java Type Parquet Type LocalDate int32 (DATE) LocalTime int32 (TIME(MILLIS|MICROS)) or int64 (TIME(NANOS)) LocalDateTime int64 (TIMESTAMP(MILLIS|MICROS|NANOS)) Instant int64 (TIMESTAMP(MILLIS|MICROS|NANOS))"},{"location":"advanced/data-types/#decimal-numbers","title":"Decimal Numbers","text":"<p>BigDecimal mapping depends on precision:</p> Precision Parquet Type \u2264 9 int32 (DECIMAL) \u2264 18 int64 (DECIMAL) &gt; 18 binary (DECIMAL) or fixed_len_byte_array (DECIMAL)"},{"location":"advanced/data-types/#nested-structures","title":"Nested Structures","text":"<p>Parquet supports nested structures, and Carpet can generate them using Java records and Collections.</p> <p>Carpet uses the following rules to generate nested structures:</p> <ol> <li>Types must be concrete and cannot be generic.</li> <li>Types cannot be recursive directly or indirectly.</li> </ol>"},{"location":"advanced/data-types/#nested-records","title":"Nested Records","text":"<p>Carpet supports nested records to create files with structured data. There is one exception: types can not be recursive directly nor indirectly.</p> <pre><code>record Address(String street, String zip, String city) { }\nrecord Job(String company, String position, int years) { }\nrecord Person(long id, Job job, Address address) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Person.class)) {\n    var president = new Person(1010101, new Job(\"USA\", POTUS, 3),\n        new Address(\"1600 Pennsylvania Av.\", \"20500\", \"Washington\"));\n    writer.write(president));\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Person {\n  required int64 id;\n  optional group job {\n    optional binary company (STRING);\n    optional binary position (STRING);\n    required int32 years;\n  }\n  optional group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre>"},{"location":"advanced/data-types/#collections","title":"Collections","text":"<p>Carpet supports nested collections to create files with structured data. Collection elements must be one of the supported types.</p> <pre><code>record Line(String sku, int quantity, double price){ }\nrecord Invoice(String id, double amount, double taxes, List&lt;Line&gt; lines) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Invoice.class)) {\n    var invoice = new Invoice(\"2023/211\", 2323.23, 232.32, List.of(\n        new Line(\"AAA\", 3, 500.0), new Line(\"BBB\", 1, 823.23)));\n    writer.write(invoice);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a collection to any type of Java <code>Collection</code> implementation. The only restriction is that the Collection type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#maps","title":"Maps","text":"<p>Carpet supports nested maps to create files with structured data. Map elements must be one of the supported types.</p> <pre><code>record State(double area, int population){ }\nrecord Country(String name, double area, Map&lt;String, State&gt; states) { }\n\ntry (var writer = new CarpetWriter&lt;&gt;(outputFile, Country.class)) {\n    var country = new Country(\"USA\", 9_833_520.0, Map.of(\n        \"Idaho\", new State(216_444.0, 1_975_000),\n        \"Texas\", new State(695_662.0, 29_145_505)));\n    writer.write(country);\n}\n</code></pre> <p>The generated file has this Parquet schema:</p> <pre><code>message Country {\n  optional binary name (STRING);\n  required double area;\n  optional group states (MAP) {\n    repeated group key_value {\n      required binary key (STRING);\n      optional group value {\n        required double area;\n        required int32 population;\n      }\n    }\n  }\n}\n</code></pre> <p>You can deserialize an existing file with a map to any type of Java <code>Map</code> implementation. The only restriction is that the Map type must have a constructor without parameters.</p>"},{"location":"advanced/data-types/#limitations","title":"Limitations","text":""},{"location":"advanced/data-types/#generic-types","title":"Generic Types","text":"<p>Records cannot have generic elements as Carpet needs concrete type information to create the Parquet schema. This includes generic records, collections, and maps.</p> <pre><code>// This will NOT work\nrecord WithGeneric&lt;T&gt;(String name, T child) { }\n\n// This works fine\nrecord WithList(String name, List&lt;String&gt; items) { }\nrecord WithMap(String name, Map&lt;String, Integer&gt; values) { }\n</code></pre> <p>If generic type is used, <code>RecordTypeConversionException</code> will be thrown.</p> <p>Collections and Maps with concrete types don't have this issue because Carpet knows their concrete type information at compile time.</p>"},{"location":"advanced/data-types/#recursive-types","title":"Recursive Types","text":"<p>Records cannot have direct or indirect recursive references:</p> <pre><code>// This will NOT work - direct recursion\nrecord Node(String id, Node next) { }\n\n// This will NOT work - indirect recursion\nrecord Child(String id, Parent parent) { }\nrecord Parent(String id, Child child) { }\n</code></pre>"},{"location":"advanced/input-output-files/","title":"Input and Output Files","text":"<p><code>parquet-java</code> defines <code>OutputFile</code> and <code>InputFile</code> interfaces to interact with files. Originally, it only provided <code>HadoopOutputFile</code> and <code>HadoopInputFile</code> implementations that were capable of working with Hadoop and local files.</p> <p>This required a Hadoop dependency to be included in the project. This is not ideal for projects that only need to work with local files, as it adds unnecessary complexity and size to the project. To address this, Parquet Java recently added <code>LocalOutputFile</code> and <code>LocalInputFile</code> implementations.</p> <p>Before these classes were created, Carpet provided a local file implementation with <code>FileSystemOutputFile</code> and <code>FileSystemInputFile</code>. You can use either implementation.</p>"},{"location":"advanced/low-level-parquet/","title":"Low level Parquet classes","text":"<p>Carpet is built on top of parquet-java library and supports creating native library <code>ParquetWriter</code> and <code>ParquetReader</code> classes, and use it with third party libraries that work with Parquet classes.</p>"},{"location":"advanced/low-level-parquet/#parquetwriter","title":"ParquetWriter","text":"<pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\nPath path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nOutputFile outputFile = HadoopOutputFile.fromPath(path, new Configuration());\ntry (ParquetWriter&lt;MyRecord&gt; writer = CarpetParquetWriter.builder(outputFile, MyRecord.class)\n        .withWriteMode(Mode.OVERWRITE)\n        .withCompressionCodec(CompressionCodecName.GZIP)\n        .withPageRowCountLimit(100_000)\n        .withBloomFilterEnabled(\"name\", true)\n        .build()) {\n\n    otherLibraryIntegrationWrite(writer, data);\n}\n</code></pre>"},{"location":"advanced/low-level-parquet/#parquetreader","title":"ParquetReader","text":"<pre><code>Path path = new org.apache.hadoop.fs.Path(\"my_file.parquet\");\nInputFile inputFile = new HadoopInputFile(path, new Configuration());\ntry (ParquetReader&lt;MyRecord&gt; reader = CarpetParquetReader.builder(inputFile, MyRecord.class).build()) {\n    var data = otherLibraryIntegrationRead(reader);\n    // process data\n}\n</code></pre>"},{"location":"advanced/nullability/","title":"Nullability","text":"<p>Parquet supports to configure not null columns in the schema. Carpet, writing the schema, respects Java primitives' nullability:</p> <p>This record:</p> <pre><code>record MyRecord(long id, String name, int size, double value){ }\n</code></pre> <p>generates this schema with primitive types as <code>required</code>:</p> <pre><code>message MyRecord {\n  required int64 id;\n  optional binary name (STRING);\n  required int32 size;\n  required double value;\n}\n</code></pre> <p>while this record:</p> <pre><code>record MyRecord(Long id, String name, Integer size, Double value) { }\n</code></pre> <p>generates this schema with all numeric values as <code>optional</code>:</p> <pre><code>message MyRecord {\n  optional int64 id;\n  optional binary name (STRING);\n  optional int32 size;\n  optional double value;\n}\n</code></pre> <p>String, List or Map types are objects and can be nullable. To generate a schema where an object reference field is created as <code>required</code> you must annotate the field with <code>@NotNull</code> annotation.</p> <pre><code>record MyRecord(@NotNull String id, @NotNull String name, @NotNull Address address){ }\n</code></pre> <p>generates this schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  required group address {\n    optional binary street (STRING);\n    optional binary zip (STRING);\n    optional binary city (STRING);\n  }\n}\n</code></pre> <p>The <code>@NotNull</code> annotation is not part of the Java standard library and Carpet provides one implementation. You can use any library that provides this type of annotation, such as <code>javax.validation.constraints.NotNull</code> or <code>jakarta.annotation.Nonnull</code>. Carpet inspects fields annotation looking by the name of the annotation not the complete type.</p>"},{"location":"advanced/projections/","title":"Projections","text":"<p>One of key features of Parquet is that you can save a lot of I/O and CPU if you read only a subset of available columns in a file.</p> <p>Given a parquet file, you can read a subset of columns just using a Record with needed columns.</p> <p>For example, from a file with this schema, you can read just id, sku, and quantity fields:</p> <pre><code>message Invoice {\n  optional binary id (STRING);\n  required double amount;\n  required double taxes;\n  optional group lines (LIST) {\n    repeated group list {\n      optional group element {\n        optional binary sku (STRING);\n        required int32 quantity;\n        required double price;\n      }\n    }\n  }\n}\n</code></pre> <p>defining this records:</p> <pre><code>record LineRead(String sku, int quantity) { }\n\nrecord InvoiceRead(String id, List&lt;LineRead&gt; lines) { }\n\nList&lt;InvoiceRead&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), InvoiceRead.class).toList();\n</code></pre> <p>Parquet will read and parse only pages with id, sku, and quantity columns, skipping the rest of the file.</p>"},{"location":"advanced/schema-mismatch/","title":"Read Schema mismatch","text":"<p>How does Carpet behave when the schema does not exactly match records types?</p>"},{"location":"advanced/schema-mismatch/#nullable-column-mapped-to-primitive-type","title":"Nullable column mapped to primitive type","text":"<p>By default Carpet doesn't fail when a column is defined as <code>optional</code> but the record field is primitive.</p> <p>This parquet schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n  optional int32 age;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>When a null value appears in a file, the field is filled with the default value of the primitive (0, 0.0 or false).</p> <p>If you want to ensure that the application fails if an optional column is mapped to a primitive field, you must enable the flag <code>FailOnNullForPrimitives</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnNullForPrimitives(true)\n    .toList();\n</code></pre> <p>By default, <code>FailOnNullForPrimitives</code> value is false.</p>"},{"location":"advanced/schema-mismatch/#missing-fields","title":"Missing fields","text":"<p>When parquet file schema doesn't match with existing record fields, Carpet throws an exception.</p> <p>This schema:</p> <pre><code>message MyRecord {\n  required binary id (STRING);\n  required binary name (STRING);\n}\n</code></pre> <p>is not compatible with this record because it contains an additional <code>int age</code> field:</p> <pre><code>record MyRecord(String id, String name, int age) { }\n</code></pre> <p>If for some reason you are forced to read the file with an incompatible record, you can disable the schema compatibility check with flag <code>FailOnMissingColumn</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailOnMissingColumn(false)\n    .toList();\n</code></pre> <p>Carpet will skip the schema verification and fill the value with <code>null</code> in case of Objects or the default value of primitives (0, 0.0 or false).</p> <p>By default, <code>FailOnMissingColumn</code> value is true.</p> <p>If a column that exists in the file is not present in the record, Carpet will ignore it and will not throw an exception because it's considered a projection.</p>"},{"location":"advanced/schema-mismatch/#narrowing-numeric-values","title":"Narrowing numeric values","text":"<p>By default Carpet converts between numeric types:</p> <ul> <li>Any integer type can be converted to another integer type of different size: byte &lt;-&gt; short &lt;-&gt; int &lt;-&gt; long.</li> <li>Any decimal type can be converted to another decimal type of different size: float &lt;-&gt; double</li> </ul> <p>This schema</p> <pre><code>message MyRecord {\n  required int64 id;\n  required double value;\n}\n</code></pre> <p>is compatible with this record:</p> <pre><code>record MyRecord(int id, float value) { }\n</code></pre> <p>Carpet will cast numeric types using Narrowing Primitive Conversion rules from Java.</p> <p>If you want to ensure that the application fails if a type is converted to a narrow value, you can enable the flag <code>FailNarrowingPrimitiveConversion</code>:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(file, MyRecord.class)\n    .withFailNarrowingPrimitiveConversion(true)\n    .toList();\n</code></pre> <p>By default, <code>FailNarrowingPrimitiveConversion</code> value is false.</p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":""},{"location":"getting-started/basic-usage/#creating-records","title":"Creating Records","text":"<p>To use Carpet, start by defining your data structure using Java records. You don't need to generate classes or inherit from Carpet classes:</p> <pre><code>record MyRecord(long id, String name, int size, double value, double percentile) { }\n</code></pre> <p>Carpet provides a writer and a reader with a default configuration and convenience methods.</p>"},{"location":"getting-started/basic-usage/#writing-to-parquet","title":"Writing to Parquet","text":"<p>Carpet can use reflection to define the Parquet file schema and writes all the content of your objects into the file:</p> <pre><code>List&lt;MyRecord&gt; data = calculateDataToPersist();\n\ntry (OutputStream outputStream = new FileOutputStream(\"my_file.parquet\")) {\n    try (CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class)) {\n        writer.write(data);\n    }\n}\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-from-parquet","title":"Reading from Parquet","text":"<p>To read a Parquet file, you just need to provide a File and Record class that matches the Parquet schema:</p> <pre><code>List&lt;MyRecord&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#reading-as-map","title":"Reading as Map","text":"<p>If you don't know the schema of the file, or a Map is valid for your use case, you can deserialize to <code>Map&lt;String, Object&gt;</code>:</p> <pre><code>List&lt;Map&gt; data = new CarpetReader&lt;&gt;(new File(\"my_file.parquet\"), Map.class).toList();\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, you can explore more advanced features:</p> <ul> <li>Writer API for detailed write operations</li> <li>Reader API for reading capabilities</li> <li>Data Types for supported data types and nested structures</li> <li>Configuration for customizing Parquet settings</li> </ul>"},{"location":"getting-started/carpetreader-api/","title":"CarpetReader API","text":"<p><code>CarpetReader</code> provides multiple ways to read data from Parquet files. When you instantiate a <code>CarpetReader</code> the file is not opened or read. It's processed when you execute one of its read methods.</p> <p>To instantiate it you need to provide a Java <code>File</code> or a Parquet <code>InputFile</code> and the class of the record you want to read. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetReader&lt;MyRecord&gt; reader = new CarpetReader&lt;&gt;(inputFile, MyRecord.class);\n</code></pre> <p>Parquet doesn't support <code>InputStream</code> because Parquet's file format requires random access to read metadata from the footer and data pages throughout the file. Since <code>InputStream</code> only provides sequential forward-only access, it's not suitable for reading Parquet files.</p>"},{"location":"getting-started/carpetreader-api/#reading-methods","title":"Reading Methods","text":""},{"location":"getting-started/carpetreader-api/#stream-processing","title":"Stream Processing","text":"<pre><code>Stream&lt;T&gt; stream()\n</code></pre> <p><code>CarpetReader&lt;T&gt;</code> can return a Java stream to iterate it applying functional logic to filter and transform its content.</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nList&lt;OtherType&gt; list = reader.stream()\n    .filter(r -&gt; r.value() &gt; 100.0)\n    .map(this::mapToOtherType)\n    .toList();\n</code></pre> <p>File content is read while streaming, not loaded entirely into memory. This is useful for large files. The stream will be closed automatically when the processing is done.</p>"},{"location":"getting-started/carpetreader-api/#collecting-tolist","title":"Collecting <code>toList</code>","text":"<p>If you don't need to filter or convert the content, you can directly collect the whole content as a <code>List&lt;T&gt;</code>:</p> <pre><code>List&lt;MyRecord&gt; list = new CarpetReader&lt;&gt;(file, MyRecord.class).toList();\n</code></pre>"},{"location":"getting-started/carpetreader-api/#for-each-loop","title":"For-Each Loop","text":"<p><code>CarpetReader&lt;T&gt;</code> implements <code>Iterable&lt;T&gt;</code> and thanks to For-Each Loop feature from Java sintax you can iterate it with a simple for:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nfor (MyRecord r: reader) {\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetreader-api/#iterator","title":"Iterator","text":"<p>Implementing <code>Iterable&lt;T&gt;</code>, there is also available a method <code>iterator()</code>:</p> <pre><code>var reader = new CarpetReader&lt;&gt;(file, MyRecord.class);\nIterator&lt;MyRecord&gt; iterator = reader.iterator();\nwhile (iterator.hasNext()) {\n    MyRecord r = iterator.next();\n    doSomething(r);\n}\n</code></pre>"},{"location":"getting-started/carpetwriter-api/","title":"CarpetWriter API","text":"<p><code>CarpetWriter</code> provides multiple methods for writing data to Parquet files.</p> <p>To instantiate it you need to provide an <code>OutputStream</code> or a Parquet <code>OutputFile</code> and the class of the record you want to write. The record class must be a Java record that match the field names in the Parquet schema.</p> <pre><code>CarpetWriter&lt;MyRecord&gt; writer = new CarpetWriter&lt;&gt;(outputStream, MyRecord.class);\n</code></pre>"},{"location":"getting-started/carpetwriter-api/#writing-methods","title":"Writing Methods","text":"<ul> <li><code>void write(T value)</code>: Write a single element. Can be called repeatedly.</li> <li><code>void accept(T value)</code>: Implementing <code>Consumer&lt;T&gt;</code> interface, write a single element. Created to be used in functional processes. If there is an <code>IOException</code>, it is wrapped with a <code>UncheckedIOException</code></li> <li><code>void write(Collection&lt;T&gt; collection)</code>: iterates and serializes a whole collection. Can be any type of <code>Collection</code> implementation.</li> <li><code>void write(Stream&lt;T&gt; stream)</code>: consumes a stream and serializes its values.</li> </ul> <p>You can call repeatedly to all methods in any combination if needed.</p>"},{"location":"getting-started/carpetwriter-api/#usage-example","title":"Usage Example","text":"<pre><code>var outputFile = new FileSystemOutputFile(new File(\"my_file.parquet\"));\ntry (var writer = new CarpetWriter&lt;MyRecord&gt;(outputStream, MyRecord.class)) {\n    // Write single element\n    writer.write(new MyRecord(\"foo\"));\n\n    // Write collection\n    writer.write(List.of(new MyRecord(\"bar\")));\n\n    // Write stream\n    writer.write(Stream.of(new MyRecord(\"foobar\")));\n}\n</code></pre> <p><code>CarpetWriter</code> needs to be closed, and implements <code>Closeable</code> interface to be used in try-with-resources.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#maven","title":"Maven","text":"<p>Include Carpet in your Java project using Maven by adding this dependency to your <code>pom.xml</code>:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.jerolba&lt;/groupId&gt;\n    &lt;artifactId&gt;carpet-record&lt;/artifactId&gt;\n    &lt;version&gt;0.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/installation/#gradle","title":"Gradle","text":"<p>If you're using Gradle, add this to your <code>build.gradle</code>:</p> <pre><code>implementation 'com.jerolba:carpet-record:0.3.0'\n</code></pre>"},{"location":"getting-started/installation/#transitive-dependencies-and-hadoop","title":"Transitive dependencies and Hadoop","text":"<p>Carpet is designed to work with local filesystems by default and includes only the minimal Parquet-related dependencies needed for read/write operations.</p> <p>While Parquet was originally developed as part of the Hadoop ecosystem, Carpet explicitly excludes most Hadoop-related transitive dependencies to keep the library lightweight.</p> <p>If you need Hadoop functionality (like HDFS support), you'll need to explicitly add Hadoop dependencies to your project.</p>"}]}